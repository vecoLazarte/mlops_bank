{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45da4993-71c1-4c4d-91e8-e6775d0b9afc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-29T17:29:01.816751Z",
     "iopub.status.busy": "2025-06-29T17:29:01.815536Z",
     "iopub.status.idle": "2025-06-29T17:29:03.923982Z",
     "shell.execute_reply": "2025-06-29T17:29:03.923184Z",
     "shell.execute_reply.started": "2025-06-29T17:29:01.816718Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.workflow.function_step import step\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "import sagemaker\n",
    "from sagemaker.workflow.parameters import ParameterString, ParameterInteger\n",
    "from sagemaker.workflow.execution_variables import ExecutionVariables\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de60c2a-bfa3-46bf-8562-f11e7c3a61cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T07:06:34.727226Z",
     "iopub.status.busy": "2025-06-28T07:06:34.726299Z",
     "iopub.status.idle": "2025-06-28T07:06:34.730687Z",
     "shell.execute_reply": "2025-06-28T07:06:34.729975Z",
     "shell.execute_reply.started": "2025-06-28T07:06:34.727193Z"
    }
   },
   "source": [
    "## GLOBAL VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dc04c20-4129-458e-8529-88a47e8bc8ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-29T17:29:03.925471Z",
     "iopub.status.busy": "2025-06-29T17:29:03.925110Z",
     "iopub.status.idle": "2025-06-29T17:29:04.225153Z",
     "shell.execute_reply": "2025-06-29T17:29:04.223999Z",
     "shell.execute_reply.started": "2025-06-29T17:29:03.925448Z"
    }
   },
   "outputs": [],
   "source": [
    "user = utils.get_username()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "default_bucket = \"mlops-chester\"\n",
    "default_prefix = \"sagemaker/bank-attrition-detection\"\n",
    "default_path = default_bucket + \"/\" + default_prefix\n",
    "sagemaker_session = sagemaker.Session(default_bucket=default_bucket,\n",
    "                                      default_bucket_prefix=default_prefix)\n",
    "\n",
    "instance_type = \"ml.m5.2xlarge\"\n",
    "pipeline_name = \"pipeline-inference\"\n",
    "model_version = \"latest\"\n",
    "model_name = \"attrition-detection-model\"\n",
    "cod_month = ParameterString(name=\"PeriodoCargaClientes\")\n",
    "cod_month_start = ParameterInteger(name=\"PeriodoCargaRequerimientosInicio\")\n",
    "cod_month_end = ParameterInteger(name=\"PeriodoCargaRequerimientosFin\")\n",
    "\n",
    "tracking_server_arn = 'arn:aws:sagemaker:us-east-2:762233743642:mlflow-tracking-server/mlops-mlflow-server'\n",
    "experiment_name = \"pipeline-inference-attrition-detection\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c443942-28c7-43fc-84af-ac51aef9aec4",
   "metadata": {},
   "source": [
    "## Data pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17a1b14f-d49c-4ed5-a9d1-8ae70e3d1c16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-29T17:29:05.175187Z",
     "iopub.status.busy": "2025-06-29T17:29:05.174631Z",
     "iopub.status.idle": "2025-06-29T17:29:05.198438Z",
     "shell.execute_reply": "2025-06-29T17:29:05.197743Z",
     "shell.execute_reply.started": "2025-06-29T17:29:05.175164Z"
    }
   },
   "outputs": [],
   "source": [
    "@step(\n",
    "    name=\"DataPull\",\n",
    "    instance_type=instance_type\n",
    ")\n",
    "def data_pull(experiment_name: str, run_name: str, cod_month: str, cod_month_start: str, cod_month_end: str) -> str:\n",
    "    import mlflow\n",
    "    from mlflow.artifacts import download_artifacts\n",
    "    import subprocess\n",
    "    subprocess.run(['pip', 'install', 'awswrangler==3.12.0']) \n",
    "    import awswrangler as wr\n",
    "    import os\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import tempfile\n",
    "    output_dir = tempfile.mkdtemp()\n",
    "    import boto3\n",
    "    s3 = boto3.client('s3')\n",
    "\n",
    "    TARGET_COL = 'ATTRITION'\n",
    "    query_clientes = \"\"\"\n",
    "       SELECT\n",
    "        TRY_CAST(id_correlativo AS BIGINT) AS id_correlativo,\n",
    "        TRY_CAST(codmes AS BIGINT) AS codmes,\n",
    "        TRY_CAST(flg_bancarizado AS BIGINT) AS flg_bancarizado,\n",
    "        rang_ingreso,\n",
    "        flag_lima_provincia,\n",
    "        TRY_CAST(edad AS DOUBLE) AS edad,\n",
    "        TRY_CAST(antiguedad AS DOUBLE) AS antiguedad,\n",
    "        rang_sdo_pasivo_menos0,\n",
    "        TRY_CAST(sdo_activo_menos0 AS BIGINT) AS sdo_activo_menos0,\n",
    "        TRY_CAST(sdo_activo_menos1 AS BIGINT) AS sdo_activo_menos1,\n",
    "        TRY_CAST(sdo_activo_menos2 AS BIGINT) AS sdo_activo_menos2,\n",
    "        TRY_CAST(sdo_activo_menos3 AS BIGINT) AS sdo_activo_menos3,\n",
    "        TRY_CAST(sdo_activo_menos4 AS BIGINT) AS sdo_activo_menos4,\n",
    "        TRY_CAST(sdo_activo_menos5 AS BIGINT) AS sdo_activo_menos5,\n",
    "        TRY_CAST(flg_seguro_menos0 AS BIGINT) AS flg_seguro_menos0,\n",
    "        TRY_CAST(flg_seguro_menos1 AS BIGINT) AS flg_seguro_menos1,\n",
    "        TRY_CAST(flg_seguro_menos2 AS BIGINT) AS flg_seguro_menos2,\n",
    "        TRY_CAST(flg_seguro_menos3 AS BIGINT) AS flg_seguro_menos3,\n",
    "        TRY_CAST(flg_seguro_menos4 AS BIGINT) AS flg_seguro_menos4,\n",
    "        TRY_CAST(flg_seguro_menos5 AS BIGINT) AS flg_seguro_menos5,\n",
    "        rang_nro_productos_menos0,\n",
    "        TRY_CAST(flg_nomina AS BIGINT) AS flg_nomina,\n",
    "        TRY_CAST(nro_acces_canal1_menos0 AS BIGINT) AS nro_acces_canal1_menos0,\n",
    "        TRY_CAST(nro_acces_canal1_menos1 AS BIGINT) AS nro_acces_canal1_menos1,\n",
    "        TRY_CAST(nro_acces_canal1_menos2 AS BIGINT) AS nro_acces_canal1_menos2,\n",
    "        TRY_CAST(nro_acces_canal1_menos3 AS BIGINT) AS nro_acces_canal1_menos3,\n",
    "        TRY_CAST(nro_acces_canal1_menos4 AS BIGINT) AS nro_acces_canal1_menos4,\n",
    "        TRY_CAST(nro_acces_canal1_menos5 AS BIGINT) AS nro_acces_canal1_menos5,\n",
    "        TRY_CAST(nro_acces_canal2_menos0 AS BIGINT) AS nro_acces_canal2_menos0,\n",
    "        TRY_CAST(nro_acces_canal2_menos1 AS BIGINT) AS nro_acces_canal2_menos1,\n",
    "        TRY_CAST(nro_acces_canal2_menos2 AS BIGINT) AS nro_acces_canal2_menos2,\n",
    "        TRY_CAST(nro_acces_canal2_menos3 AS BIGINT) AS nro_acces_canal2_menos3,\n",
    "        TRY_CAST(nro_acces_canal2_menos4 AS BIGINT) AS nro_acces_canal2_menos4,\n",
    "        TRY_CAST(nro_acces_canal2_menos5 AS BIGINT) AS nro_acces_canal2_menos5,\n",
    "        TRY_CAST(nro_acces_canal3_menos0 AS BIGINT) AS nro_acces_canal3_menos0,\n",
    "        TRY_CAST(nro_acces_canal3_menos1 AS BIGINT) AS nro_acces_canal3_menos1,\n",
    "        TRY_CAST(nro_acces_canal3_menos2 AS BIGINT) AS nro_acces_canal3_menos2,\n",
    "        TRY_CAST(nro_acces_canal3_menos3 AS BIGINT) AS nro_acces_canal3_menos3,\n",
    "        TRY_CAST(nro_acces_canal3_menos4 AS BIGINT) AS nro_acces_canal3_menos4,\n",
    "        TRY_CAST(nro_acces_canal3_menos5 AS BIGINT) AS nro_acces_canal3_menos5,\n",
    "        TRY_CAST(nro_entid_ssff_menos0 AS BIGINT) AS nro_entid_ssff_menos0,\n",
    "        TRY_CAST(nro_entid_ssff_menos1 AS BIGINT) AS nro_entid_ssff_menos1,\n",
    "        TRY_CAST(nro_entid_ssff_menos2 AS BIGINT) AS nro_entid_ssff_menos2,\n",
    "        TRY_CAST(nro_entid_ssff_menos3 AS BIGINT) AS nro_entid_ssff_menos3,\n",
    "        TRY_CAST(nro_entid_ssff_menos4 AS BIGINT) AS nro_entid_ssff_menos4,\n",
    "        TRY_CAST(nro_entid_ssff_menos5 AS BIGINT) AS nro_entid_ssff_menos5,\n",
    "        TRY_CAST(flg_sdo_otssff_menos0 AS BIGINT) AS flg_sdo_otssff_menos0,\n",
    "        TRY_CAST(flg_sdo_otssff_menos1 AS BIGINT) AS flg_sdo_otssff_menos1,\n",
    "        TRY_CAST(flg_sdo_otssff_menos2 AS BIGINT) AS flg_sdo_otssff_menos2,\n",
    "        TRY_CAST(flg_sdo_otssff_menos3 AS BIGINT) AS flg_sdo_otssff_menos3,\n",
    "        TRY_CAST(flg_sdo_otssff_menos4 AS BIGINT) AS flg_sdo_otssff_menos4,\n",
    "        TRY_CAST(flg_sdo_otssff_menos5 AS BIGINT) AS flg_sdo_otssff_menos5\n",
    "        FROM oot_clientes_sample\n",
    "        WHERE codmes = '{}';\n",
    "    \"\"\".format(cod_month)\n",
    "    \n",
    "    query_requerimientos = \"\"\"\n",
    "        SELECT *\n",
    "        FROM oot_requerimientos_sample\n",
    "        WHERE codmes between {} and {};\n",
    "        \"\"\".format(cod_month_start, cod_month_end)\n",
    "    \n",
    "    train_s3_path = f\"s3://{default_path}\"\n",
    "\n",
    "    mlflow.set_tracking_uri(tracking_server_arn)\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "\n",
    "    def prepare_impute_missing(df_data, x_cols):\n",
    "        df_data_imputed = df_data.copy()\n",
    "        s3_key = f'{default_prefix}/outputs/preprocess/imputacion_parametros.csv'\n",
    "        local_path = 'imputacion_parametros.csv'\n",
    "        s3.download_file(default_bucket, s3_key, local_path)\n",
    "        df_impute_parameters = pd.read_csv(local_path)\n",
    "        for col in x_cols:\n",
    "            impute_value = df_impute_parameters[df_impute_parameters[\"variable\"]==col][\"valor\"].values[0]\n",
    "            df_data_imputed[col] = df_data_imputed[col].fillna(impute_value)\n",
    "        return df_data_imputed\n",
    "       \n",
    "    def generar_variables_ingenieria(clientes_df):\n",
    "        clientes_df[\"VAR_SDO_ACTIVO_6M\"] = clientes_df[\"SDO_ACTIVO_MENOS0\"] - clientes_df[\"SDO_ACTIVO_MENOS5\"]\n",
    "        clientes_df[\"PROM_SDO_ACTIVO_0M_2M\"] = clientes_df[[f\"SDO_ACTIVO_MENOS{i}\" for i in range(3)]].mean(axis=1)\n",
    "        clientes_df[\"PROM_SDO_ACTIVO_3M_5M\"] = clientes_df[[f\"SDO_ACTIVO_MENOS{i}\" for i in range(3, 6)]].mean(axis=1)\n",
    "        clientes_df[\"VAR_SDO_ACTIVO_3M\"] = clientes_df[\"PROM_SDO_ACTIVO_0M_2M\"] - clientes_df[\"PROM_SDO_ACTIVO_3M_5M\"]\n",
    "        clientes_df[\"PROM_SDO_ACTIVO_6M\"] = clientes_df[[f\"SDO_ACTIVO_MENOS{i}\" for i in range(6)]].mean(axis=1)\n",
    "        clientes_df[\"MESES_CON_SEGURO\"] = clientes_df[[f\"FLG_SEGURO_MENOS{i}\" for i in range(6)]].sum(axis=1)\n",
    "        for canal in [1, 2, 3]:\n",
    "            base = f\"NRO_ACCES_CANAL{canal}_MENOS\"\n",
    "            clientes_df[f\"VAR_NRO_ACCES_CANAL{canal}_6M\"] = clientes_df[f\"{base}0\"] - clientes_df[f\"{base}5\"]\n",
    "            clientes_df[f\"PROM_NRO_ACCES_CANAL{canal}_6M\"] = clientes_df[[f\"{base}{i}\" for i in range(6)]].mean(axis=1)\n",
    "            clientes_df[f\"PROM_NRO_ACCES_CANAL{canal}_0M_2M\"] = clientes_df[[f\"{base}{i}\" for i in range(3)]].mean(axis=1)\n",
    "            clientes_df[f\"PROM_NRO_ACCES_CANAL{canal}_3M_5M\"] = clientes_df[[f\"{base}{i}\" for i in range(3, 6)]].mean(axis=1)\n",
    "            clientes_df[f\"VAR_NRO_ACCES_CANAL{canal}_3M\"] = (clientes_df[f\"PROM_NRO_ACCES_CANAL{canal}_0M_2M\"] - clientes_df[f\"PROM_NRO_ACCES_CANAL{canal}_3M_5M\"])\n",
    "        clientes_df[\"PROM_NRO_ENTID_SSFF_6M\"] = clientes_df[[f\"NRO_ENTID_SSFF_MENOS{i}\" for i in range(6)]].mean(axis=1)\n",
    "        clientes_df[\"VAR_NRO_ENTID_SSFF_6M\"] = clientes_df[\"NRO_ENTID_SSFF_MENOS0\"] - clientes_df[\"NRO_ENTID_SSFF_MENOS5\"]\n",
    "        clientes_df[\"PROM_NRO_ENTID_SSFF_0M_2M\"] = clientes_df[[f\"NRO_ENTID_SSFF_MENOS{i}\" for i in range(3)]].mean(axis=1)\n",
    "        clientes_df[\"PROM_NRO_ENTID_SSFF_3M_5M\"] = clientes_df[[f\"NRO_ENTID_SSFF_MENOS{i}\" for i in range(3, 6)]].mean(axis=1)\n",
    "        clientes_df[\"VAR_NRO_ENTID_SSFF_3M\"] = (clientes_df[\"PROM_NRO_ENTID_SSFF_0M_2M\"] - clientes_df[\"PROM_NRO_ENTID_SSFF_3M_5M\"])\n",
    "        clientes_df[\"MESES_CON_SALDO\"] = clientes_df[[f\"FLG_SDO_OTSSFF_MENOS{i}\" for i in range(6)]].sum(axis=1)\n",
    "        return clientes_df\n",
    "\n",
    "    def construir_variables_requerimientos(df_reqs, id_col='ID_CORRELATIVO'):\n",
    "        total_reqs = df_reqs.groupby(id_col).size().rename('total_requerimientos')\n",
    "        if not isinstance(total_reqs, pd.DataFrame):\n",
    "            total_reqs = total_reqs.to_frame()\n",
    "        n_tipo_req = df_reqs.groupby(id_col)['TIPO_REQUERIMIENTO2'].nunique().rename('nro_tipos_requerimiento').to_frame()\n",
    "        n_dictamen = df_reqs.groupby(id_col)['DICTAMEN'].nunique().rename('nro_dictamenes').to_frame()\n",
    "        n_producto = df_reqs.groupby(id_col)['PRODUCTO_SERVICIO_2'].nunique().rename('nro_productos_servicios').to_frame()\n",
    "        n_submotivo = df_reqs.groupby(id_col)['SUBMOTIVO_2'].nunique().rename('nro_submotivos').to_frame()\n",
    "        tipo_ohe = pd.get_dummies(df_reqs['TIPO_REQUERIMIENTO2'], prefix='tipo')\n",
    "        tipo_ohe[id_col] = df_reqs[id_col]\n",
    "        tipo_ohe = tipo_ohe.groupby(id_col).sum()\n",
    "        dictamen_ohe = pd.get_dummies(df_reqs['DICTAMEN'], prefix='dictamen')\n",
    "        dictamen_ohe[id_col] = df_reqs[id_col]\n",
    "        dictamen_ohe = dictamen_ohe.groupby(id_col).sum()\n",
    "        df_agregado = pd.concat([total_reqs, n_tipo_req, n_dictamen, n_producto, n_submotivo, tipo_ohe, dictamen_ohe],axis=1)\n",
    "        return df_agregado\n",
    "    \n",
    "    def apply_label_encoders_to_test(df_test):\n",
    "        df_test['RANG_SDO_PASIVO_MENOS0'] = df_test['RANG_SDO_PASIVO_MENOS0'].replace('Cero', 'Rango_SDO_00')\n",
    "        df_test['FLAG_LIMA_PROVINCIA'] = df_test['FLAG_LIMA_PROVINCIA'].map({'Lima': 1, 'Provincia': 0})\n",
    "        s3_key = f'{default_prefix}/outputs/preprocess/label_encoder_train.pkl'\n",
    "        local_path = 'label_encoder_train.pkl'\n",
    "        s3.download_file(default_bucket, s3_key, local_path)\n",
    "        with open(local_path, 'rb') as f:\n",
    "            encoders_clientes = pickle.load(f)\n",
    "        for col, le in encoders_clientes.items():\n",
    "            df_test[col] = le.transform(df_test[col])\n",
    "        return df_test\n",
    "    \n",
    "    def aplicar_estandarizacion_test(df_test):\n",
    "        s3_key = f'{default_prefix}/outputs/preprocess/scaler_train.pkl'\n",
    "        local_path = 'scaler_train.pkl'\n",
    "        s3.download_file(default_bucket, s3_key, local_path)\n",
    "        with open(local_path, 'rb') as f:\n",
    "            scaler = pickle.load(f)\n",
    "        no_escalar = ['ID_CORRELATIVO', 'CODMES']\n",
    "        columnas_a_escalar = df_test.columns.difference(no_escalar)\n",
    "        df_predictoras = df_test[columnas_a_escalar]\n",
    "        df_escaladas = pd.DataFrame(scaler.transform(df_predictoras),columns=columnas_a_escalar,index=df_test.index)\n",
    "        df_test_estandarizado = pd.concat([df_test[no_escalar], df_escaladas], axis=1)\n",
    "        return df_test_estandarizado\n",
    "    \n",
    "    def prepare_dataset(df_data_test, df_requerimientos_test):\n",
    "        x_cols_clientes = ['RANG_INGRESO','FLAG_LIMA_PROVINCIA','EDAD','ANTIGUEDAD']\n",
    "        x_cols_requerimientos = ['DICTAMEN']\n",
    "        df_data_imputed_clientes = prepare_impute_missing(df_data_test, x_cols_clientes)\n",
    "        df_data_imputed_requerimientos = prepare_impute_missing(df_requerimientos_test, x_cols_requerimientos)\n",
    "        df_data_feature_clientes = generar_variables_ingenieria(df_data_imputed_clientes)\n",
    "        df_data_feature_requerimientos = construir_variables_requerimientos(df_data_imputed_requerimientos)\n",
    "        df_data_encoder_clientes = apply_label_encoders_to_test(df_data_feature_clientes)\n",
    "        df_final = df_data_encoder_clientes.merge(df_data_feature_requerimientos, on='ID_CORRELATIVO', how='left')\n",
    "        df_final.fillna(0, inplace=True)\n",
    "        df_final = aplicar_estandarizacion_test(df_final)\n",
    "        return df_final\n",
    "\n",
    "    with mlflow.start_run(run_name=run_name) as run:\n",
    "        run_id = run.info.run_id\n",
    "        with mlflow.start_run(run_name=\"DataPull\", nested=True) as data_pull:\n",
    "            data_pull_id = data_pull.info.run_id\n",
    "            df_data_test = wr.athena.read_sql_query(sql=query_clientes, database=\"bank_attrition\")\n",
    "            df_data_test.columns = df_data_test.columns.str.upper()\n",
    "            df_data_test['RANG_INGRESO'] = df_data_test['RANG_INGRESO'].replace('', np.nan)\n",
    "            df_data_test['FLAG_LIMA_PROVINCIA'] = df_data_test['FLAG_LIMA_PROVINCIA'].replace('', np.nan) \n",
    "\n",
    "            df_requerimientos_test = wr.athena.read_sql_query(sql=query_requerimientos, database=\"bank_attrition\")\n",
    "            df_requerimientos_test.columns = df_requerimientos_test.columns.str.upper()\n",
    "            df_requerimientos_test['DICTAMEN'] = df_requerimientos_test['DICTAMEN'].replace('', np.nan) \n",
    "\n",
    "            df_data_score_prepared = prepare_dataset(df_data_test, df_requerimientos_test)\n",
    "            \n",
    "            df_data_score_prepared.to_csv(os.path.join(train_s3_path, \"inf-raw-data\", f\"df_data_score_prepared_{cod_month}.csv\"), index=False)\n",
    "            df_data_score_prepared.to_csv(os.path.join(output_dir, f\"df_data_score_prepared_{cod_month}.csv\"), index=False)\n",
    "            mlflow.log_artifact(os.path.join(output_dir, f\"df_data_score_prepared_{cod_month}.csv\"), artifact_path=f\"inf-raw-data\")\n",
    "            \n",
    "    return run_id, data_pull_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178596fd-562f-4c44-8db2-ced17b6d27b3",
   "metadata": {},
   "source": [
    "## Model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aadf22eb-00e5-412a-9274-97fe16441f8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-29T17:29:06.406530Z",
     "iopub.status.busy": "2025-06-29T17:29:06.406052Z",
     "iopub.status.idle": "2025-06-29T17:29:06.413926Z",
     "shell.execute_reply": "2025-06-29T17:29:06.413016Z",
     "shell.execute_reply.started": "2025-06-29T17:29:06.406503Z"
    }
   },
   "outputs": [],
   "source": [
    "@step(\n",
    "    name=\"ModelInference\",\n",
    "    instance_type=instance_type\n",
    ")\n",
    "def model_inference(experiment_name: str, run_id: str, data_pull_id: str, cod_month: int) -> str:\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import mlflow\n",
    "    from mlflow.artifacts import download_artifacts\n",
    "    import boto3\n",
    "    import tempfile\n",
    "\n",
    "    output_dir = tempfile.mkdtemp()\n",
    "    s3 = boto3.client(\"s3\")\n",
    "\n",
    "    train_s3_path = f\"s3://{default_path}\"\n",
    "\n",
    "    mlflow.set_tracking_uri(tracking_server_arn)\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    model_uri = f\"models:/{model_name}/{model_version}\"\n",
    "    flavors = mlflow.models.get_model_info(f\"models:/{model_name}/{model_version}\").flavors\n",
    "\n",
    "    if \"xgboost\" in flavors:\n",
    "        import mlflow.xgboost\n",
    "        model = mlflow.xgboost.load_model(model_uri)\n",
    "    elif \"sklearn\" in flavors:\n",
    "        import mlflow.sklearn\n",
    "        model = mlflow.sklearn.load_model(model_uri)\n",
    "    \n",
    "    info = mlflow.models.get_model_info(model_uri)\n",
    "    artifact_path = info.artifact_path\n",
    "    name_model = artifact_path.replace('_model', '')\n",
    "    \n",
    "    df_data_score_path = download_artifacts(run_id=data_pull_id, artifact_path=f\"inf-raw-data/df_data_score_prepared_{cod_month}.csv\")\n",
    "    df_data_score = pd.read_csv(df_data_score_path)\n",
    "\n",
    "    s3_key = f'{default_prefix}/outputs/train/feature_importance/{name_model}/feature_importance.csv'\n",
    "    local_path = 'feature_importance.csv'\n",
    "    s3.download_file(default_bucket, s3_key, local_path)\n",
    "    features = pd.read_csv(local_path)['variable'].to_list()\n",
    "\n",
    "    y_pred = model.predict_proba(df_data_score[features])\n",
    "    df_data_score['y_prob'] = y_pred[:,1]\n",
    "\n",
    "    with mlflow.start_run(run_id=run_id):\n",
    "        with mlflow.start_run(run_name=\"ModelInference\", nested=True) as model_inference:\n",
    "            model_inference_id = model_inference.info.run_id\n",
    "            df_data_score.to_csv(os.path.join(train_s3_path, \"inf-proc-data\", f\"df_data_score_prob_{cod_month}.csv\"), index=False)\n",
    "            df_data_score.to_csv(os.path.join(output_dir, f\"df_data_score_prob_{cod_month}.csv\"), index=False)\n",
    "            mlflow.log_artifact(os.path.join(output_dir, f\"df_data_score_prob_{cod_month}.csv\"), artifact_path=f\"inf-proc-data\")\n",
    "            mlflow.log_input(mlflow.data.from_pandas(df_data_score, os.path.join(train_s3_path, \"inf-proc-data\", f\"df_data_score_prob_{cod_month}.csv\")), context=\"ModelInference\")\n",
    "    return model_inference_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b032cbf-d4e5-4761-9bf7-aff12933035d",
   "metadata": {},
   "source": [
    "## Data push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0cbcbd7-788f-426e-b2fb-4cc0a3bc2d6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-29T17:29:07.532296Z",
     "iopub.status.busy": "2025-06-29T17:29:07.532010Z",
     "iopub.status.idle": "2025-06-29T17:29:07.540960Z",
     "shell.execute_reply": "2025-06-29T17:29:07.540216Z",
     "shell.execute_reply.started": "2025-06-29T17:29:07.532268Z"
    }
   },
   "outputs": [],
   "source": [
    "@step(\n",
    "    name=\"DataPush\",\n",
    "    instance_type=instance_type\n",
    ")\n",
    "def data_push(experiment_name: str, run_id: str, model_inference_id: str, cod_month: str):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import mlflow\n",
    "    from mlflow.artifacts import download_artifacts\n",
    "    import subprocess\n",
    "    subprocess.run(['pip', 'install', 'awswrangler==3.12.0']) \n",
    "    import awswrangler as wr\n",
    "    import numpy as np\n",
    "    from datetime import datetime\n",
    "    import pytz\n",
    "    import tempfile\n",
    "    import os\n",
    "\n",
    "    output_dir = tempfile.mkdtemp()\n",
    "\n",
    "    ID_COL = \"ID_CORRELATIVO\"\n",
    "    TIME_COL = \"CODMES\"\n",
    "    PRED_COL = \"y_prob\"\n",
    "    train_s3_path = f\"s3://{default_path}\"\n",
    "    mlflow.set_tracking_uri(tracking_server_arn)\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "\n",
    "    df_path = download_artifacts(run_id=model_inference_id, artifact_path=f'inf-proc-data/df_data_score_prob_{cod_month}.csv')\n",
    "    df = pd.read_csv(df_path)\n",
    "    \n",
    "    df['attrition_profile'] = np.where(df[PRED_COL] >= 0.415, 'High risk',\n",
    "                                   np.where(df[PRED_COL] >= 0.285, 'Medium risk',\n",
    "                                   'Low risk'))\n",
    "\n",
    "    df['model'] = model_name\n",
    "    timezone = pytz.timezone(\"America/Lima\")\n",
    "    df['load_date'] = datetime.now(timezone).strftime(\"%Y%m%d\")\n",
    "    df['order'] = df.y_prob.rank(method='first', ascending=False).astype(int)\n",
    "\n",
    "    inf_posproc_s3_path = f\"s3://{default_path}/inf-posproc-data\"\n",
    "    inf_posproc_s3_path_partition = inf_posproc_s3_path + f'/output_{cod_month}.parquet'\n",
    "    database = 'bank_attrition'\n",
    "    table_name = database + f'.attrition_detection'\n",
    "\n",
    "    # Pushing data to S3 path\n",
    "    df = df[[ID_COL, PRED_COL, 'model','attrition_profile','load_date', 'order', TIME_COL]] \n",
    "    df.to_parquet(inf_posproc_s3_path_partition, engine='pyarrow', compression='snappy')\n",
    "\n",
    "    # Creating table\n",
    "    ddl = f\"\"\"\n",
    "    CREATE EXTERNAL TABLE IF NOT EXISTS {table_name} (\n",
    "    {ID_COL} int,\n",
    "    {PRED_COL} double,\n",
    "    model string,\n",
    "    attrition_profile string,\n",
    "    load_date string,\n",
    "    order int,\n",
    "    {TIME_COL} int\n",
    "    )\n",
    "    STORED AS parquet\n",
    "    LOCATION '{inf_posproc_s3_path}'\n",
    "    TBLPROPERTIES ('parquet.compression'='SNAPPY')\n",
    "    \"\"\"\n",
    "    query_exec_id = wr.athena.start_query_execution(sql=ddl, database=database)\n",
    "    wr.athena.wait_query(query_execution_id=query_exec_id)\n",
    "\n",
    "    with mlflow.start_run(run_id=run_id):\n",
    "        with mlflow.start_run(run_name=\"DataPush\", nested=True):\n",
    "            \n",
    "            mlflow.log_input(mlflow.data.from_pandas(df, inf_posproc_s3_path_partition),context=\"DataPush\")\n",
    "            df.to_csv(os.path.join(output_dir, f\"score_prob_{cod_month}.csv\"), index=False)\n",
    "            mlflow.log_artifact(os.path.join(output_dir, f\"score_prob_{cod_month}.csv\"), artifact_path=f\"inf-posproc-data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced7e098-b853-4f39-a316-8757e94735fc",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9863d618-9b6b-4c3e-846d-c2d349265baf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-29T17:29:08.733080Z",
     "iopub.status.busy": "2025-06-29T17:29:08.732440Z",
     "iopub.status.idle": "2025-06-29T17:29:08.750632Z",
     "shell.execute_reply": "2025-06-29T17:29:08.749828Z",
     "shell.execute_reply.started": "2025-06-29T17:29:08.733053Z"
    }
   },
   "outputs": [],
   "source": [
    "data_pull_step = data_pull(experiment_name=experiment_name,\n",
    "                           run_name=ExecutionVariables.PIPELINE_EXECUTION_ID,\n",
    "                           cod_month=cod_month,\n",
    "                           cod_month_start=cod_month_start,\n",
    "                           cod_month_end=cod_month_end)\n",
    "\n",
    "model_inference_step = model_inference(experiment_name=experiment_name,\n",
    "                                       run_id=data_pull_step[0],\n",
    "                                       data_pull_id=data_pull_step[1],\n",
    "                                       cod_month=cod_month)\n",
    "\n",
    "data_push_step = data_push(experiment_name=experiment_name,\n",
    "                            run_id=data_pull_step[0],\n",
    "                            model_inference_id=model_inference_step,\n",
    "                            cod_month=cod_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47ea04af-92ad-4850-b0db-220dac077e4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-29T17:29:12.009296Z",
     "iopub.status.busy": "2025-06-29T17:29:12.008773Z",
     "iopub.status.idle": "2025-06-29T17:29:14.091909Z",
     "shell.execute_reply": "2025-06-29T17:29:14.091237Z",
     "shell.execute_reply.started": "2025-06-29T17:29:12.009264Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 17:29:12,359 sagemaker.remote_function INFO     Uploading serialized function code to s3://sagemaker-us-east-2-762233743642/pipeline-inference/DataPull/2025-06-29-17-29-12-160/function\n",
      "2025-06-29 17:29:12,417 sagemaker.remote_function INFO     Uploading serialized function arguments to s3://sagemaker-us-east-2-762233743642/pipeline-inference/DataPull/2025-06-29-17-29-12-160/arguments\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "2025-06-29 17:29:12,615 sagemaker.remote_function INFO     Uploading serialized function code to s3://sagemaker-us-east-2-762233743642/pipeline-inference/ModelInference/2025-06-29-17-29-12-160/function\n",
      "2025-06-29 17:29:12,670 sagemaker.remote_function INFO     Uploading serialized function arguments to s3://sagemaker-us-east-2-762233743642/pipeline-inference/ModelInference/2025-06-29-17-29-12-160/arguments\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "2025-06-29 17:29:12,945 sagemaker.remote_function INFO     Uploading serialized function code to s3://sagemaker-us-east-2-762233743642/pipeline-inference/DataPush/2025-06-29-17-29-12-160/function\n",
      "2025-06-29 17:29:12,994 sagemaker.remote_function INFO     Uploading serialized function arguments to s3://sagemaker-us-east-2-762233743642/pipeline-inference/DataPush/2025-06-29-17-29-12-160/arguments\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "2025-06-29 17:29:13,396 sagemaker.remote_function INFO     Uploading serialized function code to s3://sagemaker-us-east-2-762233743642/pipeline-inference/DataPull/2025-06-29-17-29-13-396/function\n",
      "2025-06-29 17:29:13,455 sagemaker.remote_function INFO     Uploading serialized function arguments to s3://sagemaker-us-east-2-762233743642/pipeline-inference/DataPull/2025-06-29-17-29-13-396/arguments\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "2025-06-29 17:29:13,631 sagemaker.remote_function INFO     Uploading serialized function code to s3://sagemaker-us-east-2-762233743642/pipeline-inference/ModelInference/2025-06-29-17-29-13-396/function\n",
      "2025-06-29 17:29:13,686 sagemaker.remote_function INFO     Uploading serialized function arguments to s3://sagemaker-us-east-2-762233743642/pipeline-inference/ModelInference/2025-06-29-17-29-13-396/arguments\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "2025-06-29 17:29:13,739 sagemaker.remote_function INFO     Uploading serialized function code to s3://sagemaker-us-east-2-762233743642/pipeline-inference/DataPush/2025-06-29-17-29-13-396/function\n",
      "2025-06-29 17:29:13,792 sagemaker.remote_function INFO     Uploading serialized function arguments to s3://sagemaker-us-east-2-762233743642/pipeline-inference/DataPush/2025-06-29-17-29-13-396/arguments\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-2:762233743642:pipeline/pipeline-inference',\n",
       " 'ResponseMetadata': {'RequestId': 'f3af7f2b-2f3a-49ab-b1b7-e712fadeccbb',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'f3af7f2b-2f3a-49ab-b1b7-e712fadeccbb',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '86',\n",
       "   'date': 'Sun, 29 Jun 2025 17:29:14 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline(name=pipeline_name,\n",
    "                    steps=[data_pull_step,model_inference_step,data_push_step],\n",
    "                   parameters=[cod_month, cod_month_start, cod_month_end])\n",
    "pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "813ae3c2-5912-4b55-b253-c20e9c07fae7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-29T17:29:15.121146Z",
     "iopub.status.busy": "2025-06-29T17:29:15.120373Z",
     "iopub.status.idle": "2025-06-29T17:29:15.360015Z",
     "shell.execute_reply": "2025-06-29T17:29:15.359421Z",
     "shell.execute_reply.started": "2025-06-29T17:29:15.121115Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_PipelineExecution(arn='arn:aws:sagemaker:us-east-2:762233743642:pipeline/pipeline-inference/execution/kjxwd7pwefsu', sagemaker_session=<sagemaker.session.Session object at 0x7f26aee20800>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.start(parameters={\"PeriodoCargaClientes\": '202408',\n",
    "                           \"PeriodoCargaRequerimientosInicio\": 202403,\n",
    "                          \"PeriodoCargaRequerimientosFin\": 202408},\n",
    "               execution_display_name=\"test-inference-full-1\",\n",
    "               execution_description=\"Testando inferece full 1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
