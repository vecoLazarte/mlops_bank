{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d769754a-c435-4d4f-9e07-63c6a4ded115",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-29T15:24:05.802518Z",
     "iopub.status.busy": "2025-06-29T15:24:05.802155Z",
     "iopub.status.idle": "2025-06-29T15:24:08.382101Z",
     "shell.execute_reply": "2025-06-29T15:24:08.381233Z",
     "shell.execute_reply.started": "2025-06-29T15:24:05.802492Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.workflow.function_step import step\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "import sagemaker\n",
    "from sagemaker.workflow.parameters import ParameterString, ParameterInteger\n",
    "from sagemaker.workflow.execution_variables import ExecutionVariables\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.fail_step import FailStep\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c11e792f-d87c-48dc-9da5-56b014403959",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-29T15:24:08.383555Z",
     "iopub.status.busy": "2025-06-29T15:24:08.383324Z",
     "iopub.status.idle": "2025-06-29T15:24:08.387634Z",
     "shell.execute_reply": "2025-06-29T15:24:08.387049Z",
     "shell.execute_reply.started": "2025-06-29T15:24:08.383533Z"
    }
   },
   "outputs": [],
   "source": [
    "user = utils.get_username()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d45d9035-c501-4e1f-8b8a-4dcdab4e62ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-29T15:24:08.388540Z",
     "iopub.status.busy": "2025-06-29T15:24:08.388335Z",
     "iopub.status.idle": "2025-06-29T15:24:08.394846Z",
     "shell.execute_reply": "2025-06-29T15:24:08.394247Z",
     "shell.execute_reply.started": "2025-06-29T15:24:08.388523Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'luis-lazarte'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c2a157-1b94-4a8a-8745-935f2bfdabdf",
   "metadata": {},
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2607648b-8018-4040-9446-71c9a304347b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-29T15:24:08.396278Z",
     "iopub.status.busy": "2025-06-29T15:24:08.395739Z",
     "iopub.status.idle": "2025-06-29T15:24:08.681491Z",
     "shell.execute_reply": "2025-06-29T15:24:08.680603Z",
     "shell.execute_reply.started": "2025-06-29T15:24:08.396258Z"
    }
   },
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "default_bucket = \"mlops-chester\"\n",
    "default_prefix = f\"sagemaker/bank-attrition-detection\"\n",
    "default_path = default_bucket + \"/\" + default_prefix\n",
    "sagemaker_session = sagemaker.Session(default_bucket=default_bucket,\n",
    "                                      default_bucket_prefix=default_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba4eaf71-ad11-41a5-a19c-b4571f2b4015",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-29T15:24:08.683109Z",
     "iopub.status.busy": "2025-06-29T15:24:08.682683Z",
     "iopub.status.idle": "2025-06-29T15:24:08.687732Z",
     "shell.execute_reply": "2025-06-29T15:24:08.686970Z",
     "shell.execute_reply.started": "2025-06-29T15:24:08.683078Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mlops-chester/sagemaker/bank-attrition-detection'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "903ddedc-d26d-4446-9e04-01b76db3d165",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-29T15:24:09.222302Z",
     "iopub.status.busy": "2025-06-29T15:24:09.221469Z",
     "iopub.status.idle": "2025-06-29T15:24:09.226309Z",
     "shell.execute_reply": "2025-06-29T15:24:09.225251Z",
     "shell.execute_reply.started": "2025-06-29T15:24:09.222273Z"
    }
   },
   "outputs": [],
   "source": [
    "instance_type = \"ml.m5.2xlarge\"\n",
    "pipeline_name = \"pipeline-train\"\n",
    "model_name = \"attrition-detection\"\n",
    "cod_month = ParameterString(name=\"PeriodoCargaClientes\")\n",
    "cod_month_start = ParameterInteger(name=\"PeriodoCargaRequerimientosInicio\")\n",
    "cod_month_end = ParameterInteger(name=\"PeriodoCargaRequerimientosFin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "987ed2d1-b064-4186-946a-2ea20f66e877",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-29T15:24:09.740412Z",
     "iopub.status.busy": "2025-06-29T15:24:09.739859Z",
     "iopub.status.idle": "2025-06-29T15:24:09.743476Z",
     "shell.execute_reply": "2025-06-29T15:24:09.742939Z",
     "shell.execute_reply.started": "2025-06-29T15:24:09.740385Z"
    }
   },
   "outputs": [],
   "source": [
    "tracking_server_arn = 'arn:aws:sagemaker:us-east-2:762233743642:mlflow-tracking-server/mlops-mlflow-server'\n",
    "experiment_name = \"pipeline-train-attrition-detection\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69269d6e-8b2c-43a0-ba3e-0c20eede9acb",
   "metadata": {},
   "source": [
    "## Data pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbec0eda-eaa0-47f0-81ff-341adf800eeb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-29T15:24:10.887315Z",
     "iopub.status.busy": "2025-06-29T15:24:10.886847Z",
     "iopub.status.idle": "2025-06-29T15:24:11.147966Z",
     "shell.execute_reply": "2025-06-29T15:24:11.147206Z",
     "shell.execute_reply.started": "2025-06-29T15:24:10.887289Z"
    }
   },
   "outputs": [],
   "source": [
    "@step(\n",
    "    name=\"DataPull\",\n",
    "    instance_type=instance_type\n",
    ")\n",
    "def data_pull(experiment_name: str, run_name: str, cod_month: str, cod_month_start: int, cod_month_end: int) -> tuple[str, str]:\n",
    "    import mlflow\n",
    "    from mlflow.artifacts import download_artifacts\n",
    "    mlflow.set_tracking_uri(tracking_server_arn)\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    import subprocess\n",
    "    subprocess.run(['pip', 'install', 'awswrangler==3.12.0']) \n",
    "    import awswrangler as wr\n",
    "    import os\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import tempfile\n",
    "    output_dir = tempfile.mkdtemp()\n",
    "    TARGET_COL = 'ATTRITION'\n",
    "    query_clientes = \"\"\"\n",
    "       SELECT\n",
    "        TRY_CAST(id_correlativo AS BIGINT) AS id_correlativo,\n",
    "        TRY_CAST(codmes AS BIGINT) AS codmes,\n",
    "        TRY_CAST(flg_bancarizado AS BIGINT) AS flg_bancarizado,\n",
    "        rang_ingreso,\n",
    "        flag_lima_provincia,\n",
    "        TRY_CAST(edad AS DOUBLE) AS edad,\n",
    "        TRY_CAST(antiguedad AS DOUBLE) AS antiguedad,\n",
    "        TRY_CAST(attrition AS BIGINT) AS attrition,\n",
    "        rang_sdo_pasivo_menos0,\n",
    "        TRY_CAST(sdo_activo_menos0 AS BIGINT) AS sdo_activo_menos0,\n",
    "        TRY_CAST(sdo_activo_menos1 AS BIGINT) AS sdo_activo_menos1,\n",
    "        TRY_CAST(sdo_activo_menos2 AS BIGINT) AS sdo_activo_menos2,\n",
    "        TRY_CAST(sdo_activo_menos3 AS BIGINT) AS sdo_activo_menos3,\n",
    "        TRY_CAST(sdo_activo_menos4 AS BIGINT) AS sdo_activo_menos4,\n",
    "        TRY_CAST(sdo_activo_menos5 AS BIGINT) AS sdo_activo_menos5,\n",
    "        TRY_CAST(flg_seguro_menos0 AS BIGINT) AS flg_seguro_menos0,\n",
    "        TRY_CAST(flg_seguro_menos1 AS BIGINT) AS flg_seguro_menos1,\n",
    "        TRY_CAST(flg_seguro_menos2 AS BIGINT) AS flg_seguro_menos2,\n",
    "        TRY_CAST(flg_seguro_menos3 AS BIGINT) AS flg_seguro_menos3,\n",
    "        TRY_CAST(flg_seguro_menos4 AS BIGINT) AS flg_seguro_menos4,\n",
    "        TRY_CAST(flg_seguro_menos5 AS BIGINT) AS flg_seguro_menos5,\n",
    "        rang_nro_productos_menos0,\n",
    "        TRY_CAST(flg_nomina AS BIGINT) AS flg_nomina,\n",
    "        TRY_CAST(nro_acces_canal1_menos0 AS BIGINT) AS nro_acces_canal1_menos0,\n",
    "        TRY_CAST(nro_acces_canal1_menos1 AS BIGINT) AS nro_acces_canal1_menos1,\n",
    "        TRY_CAST(nro_acces_canal1_menos2 AS BIGINT) AS nro_acces_canal1_menos2,\n",
    "        TRY_CAST(nro_acces_canal1_menos3 AS BIGINT) AS nro_acces_canal1_menos3,\n",
    "        TRY_CAST(nro_acces_canal1_menos4 AS BIGINT) AS nro_acces_canal1_menos4,\n",
    "        TRY_CAST(nro_acces_canal1_menos5 AS BIGINT) AS nro_acces_canal1_menos5,\n",
    "        TRY_CAST(nro_acces_canal2_menos0 AS BIGINT) AS nro_acces_canal2_menos0,\n",
    "        TRY_CAST(nro_acces_canal2_menos1 AS BIGINT) AS nro_acces_canal2_menos1,\n",
    "        TRY_CAST(nro_acces_canal2_menos2 AS BIGINT) AS nro_acces_canal2_menos2,\n",
    "        TRY_CAST(nro_acces_canal2_menos3 AS BIGINT) AS nro_acces_canal2_menos3,\n",
    "        TRY_CAST(nro_acces_canal2_menos4 AS BIGINT) AS nro_acces_canal2_menos4,\n",
    "        TRY_CAST(nro_acces_canal2_menos5 AS BIGINT) AS nro_acces_canal2_menos5,\n",
    "        TRY_CAST(nro_acces_canal3_menos0 AS BIGINT) AS nro_acces_canal3_menos0,\n",
    "        TRY_CAST(nro_acces_canal3_menos1 AS BIGINT) AS nro_acces_canal3_menos1,\n",
    "        TRY_CAST(nro_acces_canal3_menos2 AS BIGINT) AS nro_acces_canal3_menos2,\n",
    "        TRY_CAST(nro_acces_canal3_menos3 AS BIGINT) AS nro_acces_canal3_menos3,\n",
    "        TRY_CAST(nro_acces_canal3_menos4 AS BIGINT) AS nro_acces_canal3_menos4,\n",
    "        TRY_CAST(nro_acces_canal3_menos5 AS BIGINT) AS nro_acces_canal3_menos5,\n",
    "        TRY_CAST(nro_entid_ssff_menos0 AS BIGINT) AS nro_entid_ssff_menos0,\n",
    "        TRY_CAST(nro_entid_ssff_menos1 AS BIGINT) AS nro_entid_ssff_menos1,\n",
    "        TRY_CAST(nro_entid_ssff_menos2 AS BIGINT) AS nro_entid_ssff_menos2,\n",
    "        TRY_CAST(nro_entid_ssff_menos3 AS BIGINT) AS nro_entid_ssff_menos3,\n",
    "        TRY_CAST(nro_entid_ssff_menos4 AS BIGINT) AS nro_entid_ssff_menos4,\n",
    "        TRY_CAST(nro_entid_ssff_menos5 AS BIGINT) AS nro_entid_ssff_menos5,\n",
    "        TRY_CAST(flg_sdo_otssff_menos0 AS BIGINT) AS flg_sdo_otssff_menos0,\n",
    "        TRY_CAST(flg_sdo_otssff_menos1 AS BIGINT) AS flg_sdo_otssff_menos1,\n",
    "        TRY_CAST(flg_sdo_otssff_menos2 AS BIGINT) AS flg_sdo_otssff_menos2,\n",
    "        TRY_CAST(flg_sdo_otssff_menos3 AS BIGINT) AS flg_sdo_otssff_menos3,\n",
    "        TRY_CAST(flg_sdo_otssff_menos4 AS BIGINT) AS flg_sdo_otssff_menos4,\n",
    "        TRY_CAST(flg_sdo_otssff_menos5 AS BIGINT) AS flg_sdo_otssff_menos5\n",
    "        FROM train_clientes_sample\n",
    "        WHERE codmes = '{}';\n",
    "    \"\"\".format(cod_month)\n",
    "\n",
    "    query_requerimientos = \"\"\"\n",
    "        SELECT *\n",
    "        FROM train_requerimientos\n",
    "        WHERE codmes between {} and {};\n",
    "        \"\"\".format(cod_month_start, cod_month_end)\n",
    "    \n",
    "    train_s3_path = f\"s3://{default_path}\"\n",
    "\n",
    "    # Funciones extraccion de datos\n",
    "    def buscar_indices_coincidentes(df_clientes):\n",
    "        ids_comunes = list(set(df_clientes['ID_CORRELATIVO']))\n",
    "        return ids_comunes\n",
    "\n",
    "    def split_data(ids_comunes):\n",
    "        return train_test_split(ids_comunes, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Funciones preprocesamiento\n",
    "    def save_y_col_name(y_col):\n",
    "        df_y_col_name = pd.DataFrame({'y_col':[y_col]})\n",
    "        df_y_col_name.to_csv(os.path.join(train_s3_path, \"outputs\", \"preprocess\", \"y_col_name.csv\"), index=False)\n",
    "        df_y_col_name.to_csv(os.path.join(output_dir, \"y_col_name.csv\"), index=False)\n",
    "        mlflow.log_artifact(os.path.join(output_dir, \"y_col_name.csv\"), artifact_path=\"outputs/preprocess\")\n",
    "\n",
    "    def save_x_col_names(df_final, y_col):\n",
    "        x_cols = [col for col in df_final.columns if col != y_col and col not in ['ID_CORRELATIVO', 'CODMES']]\n",
    "        df_x_col_names = pd.DataFrame({'x_col': x_cols})\n",
    "        df_x_col_names.to_csv(os.path.join(train_s3_path, \"outputs\", \"preprocess\", \"x_col_names.csv\"), index=False)\n",
    "        df_x_col_names.to_csv(os.path.join(output_dir, \"x_col_names.csv\"), index=False)\n",
    "        mlflow.log_artifact(os.path.join(output_dir, \"x_col_names.csv\"), artifact_path=\"outputs/preprocess\")\n",
    "        \n",
    "    def generar_variables_ingenieria(clientes_df):\n",
    "        clientes_df[\"VAR_SDO_ACTIVO_6M\"] = clientes_df[\"SDO_ACTIVO_MENOS0\"] - clientes_df[\"SDO_ACTIVO_MENOS5\"]\n",
    "        clientes_df[\"PROM_SDO_ACTIVO_0M_2M\"] = clientes_df[[f\"SDO_ACTIVO_MENOS{i}\" for i in range(3)]].mean(axis=1)\n",
    "        clientes_df[\"PROM_SDO_ACTIVO_3M_5M\"] = clientes_df[[f\"SDO_ACTIVO_MENOS{i}\" for i in range(3, 6)]].mean(axis=1)\n",
    "        clientes_df[\"VAR_SDO_ACTIVO_3M\"] = clientes_df[\"PROM_SDO_ACTIVO_0M_2M\"] - clientes_df[\"PROM_SDO_ACTIVO_3M_5M\"]\n",
    "        clientes_df[\"PROM_SDO_ACTIVO_6M\"] = clientes_df[[f\"SDO_ACTIVO_MENOS{i}\" for i in range(6)]].mean(axis=1)\n",
    "        clientes_df[\"MESES_CON_SEGURO\"] = clientes_df[[f\"FLG_SEGURO_MENOS{i}\" for i in range(6)]].sum(axis=1)\n",
    "        for canal in [1, 2, 3]:\n",
    "            base = f\"NRO_ACCES_CANAL{canal}_MENOS\"\n",
    "            clientes_df[f\"VAR_NRO_ACCES_CANAL{canal}_6M\"] = clientes_df[f\"{base}0\"] - clientes_df[f\"{base}5\"]\n",
    "            clientes_df[f\"PROM_NRO_ACCES_CANAL{canal}_6M\"] = clientes_df[[f\"{base}{i}\" for i in range(6)]].mean(axis=1)\n",
    "            clientes_df[f\"PROM_NRO_ACCES_CANAL{canal}_0M_2M\"] = clientes_df[[f\"{base}{i}\" for i in range(3)]].mean(axis=1)\n",
    "            clientes_df[f\"PROM_NRO_ACCES_CANAL{canal}_3M_5M\"] = clientes_df[[f\"{base}{i}\" for i in range(3, 6)]].mean(axis=1)\n",
    "            clientes_df[f\"VAR_NRO_ACCES_CANAL{canal}_3M\"] = (clientes_df[f\"PROM_NRO_ACCES_CANAL{canal}_0M_2M\"] - clientes_df[f\"PROM_NRO_ACCES_CANAL{canal}_3M_5M\"])\n",
    "        clientes_df[\"PROM_NRO_ENTID_SSFF_6M\"] = clientes_df[[f\"NRO_ENTID_SSFF_MENOS{i}\" for i in range(6)]].mean(axis=1)\n",
    "        clientes_df[\"VAR_NRO_ENTID_SSFF_6M\"] = clientes_df[\"NRO_ENTID_SSFF_MENOS0\"] - clientes_df[\"NRO_ENTID_SSFF_MENOS5\"]\n",
    "        clientes_df[\"PROM_NRO_ENTID_SSFF_0M_2M\"] = clientes_df[[f\"NRO_ENTID_SSFF_MENOS{i}\" for i in range(3)]].mean(axis=1)\n",
    "        clientes_df[\"PROM_NRO_ENTID_SSFF_3M_5M\"] = clientes_df[[f\"NRO_ENTID_SSFF_MENOS{i}\" for i in range(3, 6)]].mean(axis=1)\n",
    "        clientes_df[\"VAR_NRO_ENTID_SSFF_3M\"] = (clientes_df[\"PROM_NRO_ENTID_SSFF_0M_2M\"] - clientes_df[\"PROM_NRO_ENTID_SSFF_3M_5M\"])\n",
    "        clientes_df[\"MESES_CON_SALDO\"] = clientes_df[[f\"FLG_SDO_OTSSFF_MENOS{i}\" for i in range(6)]].sum(axis=1)\n",
    "        return clientes_df\n",
    "    \n",
    "    def imputacion_variables(clientes_df, requerimientos_df):\n",
    "        imputaciones = []\n",
    "        moda_rango = clientes_df['RANG_INGRESO'].mode()[0]\n",
    "        clientes_df['RANG_INGRESO'] = clientes_df['RANG_INGRESO'].fillna(moda_rango)\n",
    "        imputaciones.append({'dataframe': 'clientes', 'variable': 'RANG_INGRESO', 'estrategia': 'moda', 'valor': moda_rango})\n",
    "        moda_lima = clientes_df['FLAG_LIMA_PROVINCIA'].mode()[0]\n",
    "        clientes_df['FLAG_LIMA_PROVINCIA'] = clientes_df['FLAG_LIMA_PROVINCIA'].fillna(moda_lima)\n",
    "        imputaciones.append({'dataframe': 'clientes', 'variable': 'FLAG_LIMA_PROVINCIA', 'estrategia': 'moda', 'valor': moda_lima})\n",
    "        mediana_edad = clientes_df['EDAD'].median()\n",
    "        clientes_df['EDAD'] = clientes_df['EDAD'].fillna(mediana_edad)\n",
    "        imputaciones.append({'dataframe': 'clientes', 'variable': 'EDAD', 'estrategia': 'mediana', 'valor': mediana_edad})\n",
    "        mediana_antig = clientes_df['ANTIGUEDAD'].median()\n",
    "        clientes_df['ANTIGUEDAD'] = clientes_df['ANTIGUEDAD'].fillna(mediana_antig)\n",
    "        imputaciones.append({'dataframe': 'clientes', 'variable': 'ANTIGUEDAD', 'estrategia': 'mediana', 'valor': mediana_antig})\n",
    "        moda_dictamen = requerimientos_df['DICTAMEN'].mode()[0]\n",
    "        requerimientos_df['DICTAMEN'] = requerimientos_df['DICTAMEN'].fillna(moda_dictamen)\n",
    "        imputaciones.append({'dataframe': 'requerimientos', 'variable': 'DICTAMEN', 'estrategia': 'moda', 'valor': moda_dictamen})\n",
    "        df_imputaciones = pd.DataFrame(imputaciones)\n",
    "        df_imputaciones.to_csv(os.path.join(train_s3_path, \"outputs\", \"preprocess\", \"imputacion_parametros.csv\"), index=False)\n",
    "        df_imputaciones.to_csv(os.path.join(output_dir, \"imputacion_parametros.csv\"), index=False)\n",
    "        mlflow.log_artifact(os.path.join(output_dir, \"imputacion_parametros.csv\"), artifact_path=\"outputs/preprocess\")\n",
    "        return clientes_df, requerimientos_df, df_imputaciones\n",
    "\n",
    "    def encoder_categoricos(clientes_df):\n",
    "        clientes_df['RANG_SDO_PASIVO_MENOS0'] = clientes_df['RANG_SDO_PASIVO_MENOS0'].replace('Cero', 'Rango_SDO_00')\n",
    "        clientes_df['FLAG_LIMA_PROVINCIA'] = clientes_df['FLAG_LIMA_PROVINCIA'].map({'Lima': 1, 'Provincia': 0})\n",
    "        cat_cols = clientes_df.select_dtypes(include=['object', 'category','string']).columns\n",
    "        encoders_clientes = {} \n",
    "        for col in cat_cols:\n",
    "            le = LabelEncoder()\n",
    "            clientes_df[col] = le.fit_transform(clientes_df[col])\n",
    "            encoders_clientes[col] = le\n",
    "        return clientes_df, encoders_clientes\n",
    "\n",
    "    def construir_variables_requerimientos(df_reqs, id_col='ID_CORRELATIVO'):\n",
    "        total_reqs = df_reqs.groupby(id_col).size().rename('total_requerimientos')\n",
    "        if not isinstance(total_reqs, pd.DataFrame):\n",
    "            total_reqs = total_reqs.to_frame()\n",
    "        n_tipo_req = df_reqs.groupby(id_col)['TIPO_REQUERIMIENTO2'].nunique().rename('nro_tipos_requerimiento').to_frame()\n",
    "        n_dictamen = df_reqs.groupby(id_col)['DICTAMEN'].nunique().rename('nro_dictamenes').to_frame()\n",
    "        n_producto = df_reqs.groupby(id_col)['PRODUCTO_SERVICIO_2'].nunique().rename('nro_productos_servicios').to_frame()\n",
    "        n_submotivo = df_reqs.groupby(id_col)['SUBMOTIVO_2'].nunique().rename('nro_submotivos').to_frame()\n",
    "        tipo_ohe = pd.get_dummies(df_reqs['TIPO_REQUERIMIENTO2'], prefix='tipo')\n",
    "        tipo_ohe[id_col] = df_reqs[id_col]\n",
    "        tipo_ohe = tipo_ohe.groupby(id_col).sum()\n",
    "        dictamen_ohe = pd.get_dummies(df_reqs['DICTAMEN'], prefix='dictamen')\n",
    "        dictamen_ohe[id_col] = df_reqs[id_col]\n",
    "        dictamen_ohe = dictamen_ohe.groupby(id_col).sum()\n",
    "        df_agregado = pd.concat([total_reqs, n_tipo_req, n_dictamen, n_producto, n_submotivo, tipo_ohe, dictamen_ohe],axis=1)\n",
    "        return df_agregado\n",
    "\n",
    "    def estandarizacion(df_final):\n",
    "        no_escalar = ['ID_CORRELATIVO', 'CODMES', 'ATTRITION']\n",
    "        columnas_a_escalar = df_final.columns.difference(no_escalar)\n",
    "        df_predictoras = df_final[columnas_a_escalar]\n",
    "        scaler = StandardScaler()\n",
    "        df_escaladas = pd.DataFrame(scaler.fit_transform(df_predictoras),columns=columnas_a_escalar,index=df_final.index)\n",
    "        df_final_estandarizado = pd.concat([df_final[no_escalar], df_escaladas],axis=1)\n",
    "        return df_final_estandarizado, scaler\n",
    "\n",
    "    def preprocess_dataset(clientes_df, requerimientos_df, y_col):\n",
    "        save_y_col_name(y_col)\n",
    "        clientes_df = generar_variables_ingenieria(clientes_df)\n",
    "        clientes_df,requerimientos_df,df_imputaciones = imputacion_variables(clientes_df,requerimientos_df)\n",
    "        clientes_df, artifact_encoders_clientes = encoder_categoricos(clientes_df)\n",
    "        requerimientos_df = construir_variables_requerimientos(requerimientos_df)\n",
    "        df_final = clientes_df.merge(requerimientos_df, on='ID_CORRELATIVO', how='left')\n",
    "        df_final.fillna(0, inplace=True)\n",
    "        df_final, artifact_scaler = estandarizacion(df_final)\n",
    "        save_x_col_names(df_final, y_col)\n",
    "        return df_final, artifact_encoders_clientes, artifact_scaler, df_imputaciones\n",
    "\n",
    "    # Funciones preprocesamiento test\n",
    "\n",
    "    def prepare_impute_missing_test(df_data, x_cols, df_impute_parameters):\n",
    "        df_data_imputed = df_data.copy()\n",
    "        for col in x_cols:\n",
    "            impute_value = df_impute_parameters[df_impute_parameters[\"variable\"]==col][\"valor\"].values[0]\n",
    "            df_data_imputed[col] = df_data_imputed[col].fillna(impute_value)\n",
    "        return df_data_imputed\n",
    "\n",
    "    def apply_label_encoders_to_test(df_test, encoders_clientes):\n",
    "        df_test['RANG_SDO_PASIVO_MENOS0'] = df_test['RANG_SDO_PASIVO_MENOS0'].replace('Cero', 'Rango_SDO_00')\n",
    "        df_test['FLAG_LIMA_PROVINCIA'] = df_test['FLAG_LIMA_PROVINCIA'].map({'Lima': 1, 'Provincia': 0})\n",
    "        for col, le in encoders_clientes.items():\n",
    "            df_test[col] = le.transform(df_test[col])\n",
    "        return df_test\n",
    "\n",
    "    def aplicar_estandarizacion_test(df_test, scaler):\n",
    "        no_escalar = ['ID_CORRELATIVO', 'CODMES', 'ATTRITION']\n",
    "        columnas_a_escalar = df_test.columns.difference(no_escalar)\n",
    "        df_predictoras = df_test[columnas_a_escalar]\n",
    "        df_escaladas = pd.DataFrame(scaler.transform(df_predictoras),columns=columnas_a_escalar,index=df_test.index)\n",
    "        df_test_estandarizado = pd.concat([df_test[no_escalar], df_escaladas], axis=1)\n",
    "        return df_test_estandarizado\n",
    "\n",
    "    def prepare_dataset_test(df_data_test,df_requerimientos_test,df_impute_parameters,encoders_clientes,scaler):\n",
    "        x_cols_clientes = ['RANG_INGRESO','FLAG_LIMA_PROVINCIA','EDAD','ANTIGUEDAD']\n",
    "        x_cols_requerimientos = ['DICTAMEN']\n",
    "        df_data_imputed_clientes = prepare_impute_missing_test(df_data_test, x_cols_clientes, df_impute_parameters)\n",
    "        df_data_imputed_requerimientos = prepare_impute_missing_test(df_requerimientos_test, x_cols_requerimientos, df_impute_parameters)\n",
    "        df_data_feature_clientes = generar_variables_ingenieria(df_data_imputed_clientes)\n",
    "        df_data_feature_requerimientos = construir_variables_requerimientos(df_data_imputed_requerimientos)\n",
    "        df_data_encoder_clientes = apply_label_encoders_to_test(df_data_feature_clientes, encoders_clientes)\n",
    "        df_final = df_data_encoder_clientes.merge(df_data_feature_requerimientos, on='ID_CORRELATIVO', how='left')\n",
    "        df_final.fillna(0, inplace=True)\n",
    "        df_final = aplicar_estandarizacion_test(df_final, scaler)\n",
    "        return df_final\n",
    "    \n",
    "    with mlflow.start_run(run_name=run_name) as run:\n",
    "        run_id = run.info.run_id\n",
    "\n",
    "        with mlflow.start_run(run_name=\"DataPull\", nested=True) as data_pull:\n",
    "            data_pull_id = data_pull.info.run_id\n",
    "\n",
    "            # Ejecutamos funciones de extraccion de datos\n",
    "\n",
    "            df_clientes = wr.athena.read_sql_query(sql=query_clientes, database=\"bank_attrition\")\n",
    "            df_clientes.columns = df_clientes.columns.str.upper()\n",
    "            df_clientes['RANG_INGRESO'] = df_clientes['RANG_INGRESO'].replace('', np.nan)\n",
    "            df_clientes['FLAG_LIMA_PROVINCIA'] = df_clientes['FLAG_LIMA_PROVINCIA'].replace('', np.nan) \n",
    "\n",
    "            df_requerimientos = wr.athena.read_sql_query(sql=query_requerimientos, database=\"bank_attrition\")\n",
    "            df_requerimientos.columns = df_requerimientos.columns.str.upper()\n",
    "            df_requerimientos['DICTAMEN'] = df_requerimientos['DICTAMEN'].replace('', np.nan) \n",
    "            \n",
    "            ids_comunes = buscar_indices_coincidentes(df_clientes)\n",
    "            ids_train, ids_test = split_data(ids_comunes)\n",
    "\n",
    "            train_clientes = df_clientes[df_clientes['ID_CORRELATIVO'].isin(ids_train)].copy()\n",
    "            test_clientes = df_clientes[df_clientes['ID_CORRELATIVO'].isin(ids_test)].copy()\n",
    "\n",
    "            train_requerimientos = df_requerimientos[df_requerimientos['ID_CORRELATIVO'].isin(ids_train)].copy()\n",
    "            test_requerimientos = df_requerimientos[df_requerimientos['ID_CORRELATIVO'].isin(ids_test)].copy()\n",
    "\n",
    "            path_train_clientes = os.path.join(train_s3_path, \"data\", \"out\", \"clientes_data_train.csv\")\n",
    "            path_test_clientes = os.path.join(train_s3_path, \"data\", \"out\", \"clientes_data_test.csv\")\n",
    "            path_train_reqs = os.path.join(train_s3_path, \"data\", \"out\", \"requerimientos_data_train.csv\")\n",
    "            path_test_reqs = os.path.join(train_s3_path, \"data\", \"out\", \"requerimientos_data_test.csv\")\n",
    "\n",
    "            train_clientes.to_csv(path_train_clientes, index=False)\n",
    "            test_clientes.to_csv(path_test_clientes, index=False)\n",
    "            train_requerimientos.to_csv(path_train_reqs, index=False)\n",
    "            test_requerimientos.to_csv(path_test_reqs, index=False)\n",
    "\n",
    "            train_clientes.to_csv(os.path.join(output_dir, \"clientes_data_train.csv\"), index=False)\n",
    "            test_clientes.to_csv(os.path.join(output_dir, \"clientes_data_test.csv\"), index=False)\n",
    "            train_requerimientos.to_csv(os.path.join(output_dir, \"requerimientos_data_train.csv\"), index=False)\n",
    "            test_requerimientos.to_csv(os.path.join(output_dir, \"requerimientos_data_test.csv\"), index=False)\n",
    "\n",
    "            mlflow.log_artifact(os.path.join(output_dir, \"clientes_data_train.csv\"), artifact_path=\"data/out\")\n",
    "            mlflow.log_artifact(os.path.join(output_dir, \"clientes_data_test.csv\"), artifact_path=\"data/out\")\n",
    "            mlflow.log_artifact(os.path.join(output_dir, \"requerimientos_data_train.csv\"), artifact_path=\"data/out\")\n",
    "            mlflow.log_artifact(os.path.join(output_dir, \"requerimientos_data_test.csv\"), artifact_path=\"data/out\")\n",
    "\n",
    "            mlflow.log_input(mlflow.data.from_pandas(train_clientes, path_train_clientes, targets=TARGET_COL), context=\"DataPull_train_clientes\")\n",
    "            mlflow.log_input(mlflow.data.from_pandas(train_requerimientos, path_train_reqs), context=\"DataPull_train_requerimientos\")\n",
    "\n",
    "            mlflow.log_input(mlflow.data.from_pandas(test_clientes, path_test_clientes, targets=TARGET_COL), context=\"DataPull_test_clientes\")\n",
    "            mlflow.log_input(mlflow.data.from_pandas(test_requerimientos, path_test_reqs), context=\"DataPull_test_requerimientos\")\n",
    "\n",
    "            # Ejecutamos las funciones de preprocesamiento\n",
    "\n",
    "            df_data_train_prepared, artifact_encoders_clientes, artifact_scaler, df_imputaciones = preprocess_dataset(train_clientes, train_requerimientos, TARGET_COL)\n",
    "            df_data_train_prepared.to_csv(os.path.join(train_s3_path, \"data\", \"out\", \"data_train_prepared.csv\"), index=False)\n",
    "            df_data_train_prepared.to_csv(os.path.join(output_dir, \"data_train_prepared.csv\"), index=False)\n",
    "            mlflow.log_artifact(os.path.join(output_dir, \"data_train_prepared.csv\"), artifact_path=\"data/out\")\n",
    "            \n",
    "            with open(os.path.join(output_dir, \"scaler_train.pkl\"), 'wb') as f:\n",
    "                pickle.dump(artifact_scaler, f)\n",
    "            mlflow.log_artifact(os.path.join(output_dir, \"scaler_train.pkl\"), artifact_path=\"outputs/preprocess\")\n",
    "\n",
    "            with open(os.path.join(output_dir, \"label_encoder_train.pkl\"), 'wb') as f:\n",
    "                pickle.dump(artifact_encoders_clientes, f)\n",
    "            mlflow.log_artifact(os.path.join(output_dir, \"label_encoder_train.pkl\"), artifact_path=\"outputs/preprocess\")\n",
    "\n",
    "            wr.s3.upload(local_file=os.path.join(output_dir, \"scaler_train.pkl\"),path=os.path.join(train_s3_path, \"outputs\", \"preprocess\", \"scaler_train.pkl\"))\n",
    "            wr.s3.upload(local_file=os.path.join(output_dir, \"label_encoder_train.pkl\"),path=os.path.join(train_s3_path, \"outputs\", \"preprocess\", \"label_encoder_train.pkl\"))\n",
    "\n",
    "            # Ejecutamos las funciones de preprocesamiento test\n",
    "            df_data_test_prepared  = prepare_dataset_test(test_clientes,test_requerimientos,df_imputaciones,artifact_encoders_clientes,artifact_scaler)\n",
    "            df_data_test_prepared.to_csv(os.path.join(train_s3_path, \"data\", \"out\", \"data_test_prepared.csv\"), index=False)\n",
    "            df_data_test_prepared.to_csv(os.path.join(output_dir, \"data_test_prepared.csv\"), index=False)\n",
    "            mlflow.log_artifact(os.path.join(output_dir, \"data_test_prepared.csv\"), artifact_path=\"data/out\")\n",
    "    return run_id, data_pull_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffcddc3-800b-4df6-bf1b-e0d0fb348849",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17d0c50e-87d1-48fc-bedd-770c834017b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-29T15:24:12.298581Z",
     "iopub.status.busy": "2025-06-29T15:24:12.298169Z",
     "iopub.status.idle": "2025-06-29T15:24:12.313732Z",
     "shell.execute_reply": "2025-06-29T15:24:12.312906Z",
     "shell.execute_reply.started": "2025-06-29T15:24:12.298555Z"
    }
   },
   "outputs": [],
   "source": [
    "@step(\n",
    "    name=\"ModelTraining\",\n",
    "    instance_type=instance_type\n",
    ")\n",
    "def model_training(experiment_name: str, run_id: str, data_pull_id: str ) -> str:\n",
    "    import subprocess\n",
    "    subprocess.run(['pip', 'install', 'awswrangler==3.12.0']) \n",
    "    import awswrangler as wr\n",
    "    import mlflow\n",
    "    from mlflow.artifacts import download_artifacts\n",
    "    import pandas as pd\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from xgboost import XGBClassifier\n",
    "    import os\n",
    "    import pickle\n",
    "    import tempfile\n",
    "    from sklearn.metrics import classification_report, roc_auc_score, RocCurveDisplay, log_loss, ConfusionMatrixDisplay, roc_curve\n",
    "    import matplotlib.pyplot as plt\n",
    "    import mlflow.sklearn\n",
    "    import mlflow.xgboost\n",
    "    from mlflow.models.signature import infer_signature\n",
    "    from mlflow.tracking import MlflowClient\n",
    "    output_dir = tempfile.mkdtemp()\n",
    "    mlflow.set_tracking_uri(tracking_server_arn)\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    train_s3_path = f\"s3://{default_path}\"\n",
    "\n",
    "    # Funciones train CV\n",
    "    \n",
    "    def get_preprocess_x_columns():\n",
    "        x_cols_path = download_artifacts(run_id=data_pull_id, artifact_path='outputs/preprocess/x_col_names.csv')\n",
    "        x_cols = pd.read_csv(x_cols_path)['x_col'].to_list()\n",
    "        return x_cols\n",
    "\n",
    "    def get_preprocess_y_column():\n",
    "        y_col_path = download_artifacts(run_id=data_pull_id, artifact_path='outputs/preprocess/y_col_name.csv')\n",
    "        y_col = pd.read_csv(y_col_path)['y_col'].to_list()\n",
    "        return y_col\n",
    "\n",
    "    def train_evaluate_models(df_data_train, model_parameters_grid, model, name_path):\n",
    "        x_cols = get_preprocess_x_columns()\n",
    "        y_col = get_preprocess_y_column()\n",
    "        mlflow.log_param(\"set_train_cv_rows\", df_data_train.shape[0])\n",
    "        mlflow.log_param(\"set_train_cv_cols\", x_cols)\n",
    "        grid_search = GridSearchCV(estimator=model, param_grid=model_parameters_grid, scoring='roc_auc', cv=3, verbose=1, n_jobs=-1)\n",
    "        grid_search.fit(df_data_train[x_cols], df_data_train[y_col].values.ravel())\n",
    "        prefixed_params = {f\"best_param_{name_path}_{k}\": v for k, v in grid_search.best_params_.items()}\n",
    "        mlflow.log_params(prefixed_params)\n",
    "        df_model_results = pd.DataFrame({'model_parameters': grid_search.cv_results_['params'],\n",
    "                                         'model_rank': grid_search.cv_results_['rank_test_score'],\n",
    "                                         'auc_score_mean': grid_search.cv_results_['mean_test_score'],\n",
    "                                         'auc_score_std': grid_search.cv_results_['std_test_score']})\n",
    "        df_model_results['auc_score_cv'] = df_model_results['auc_score_std'] / df_model_results['auc_score_mean']\n",
    "        df_model_results.to_csv(os.path.join(train_s3_path, \"outputs\", \"train\", \"metrics\", name_path, \"train_cv_model_results.csv\"), index=False)\n",
    "        df_model_results.to_csv(os.path.join(output_dir, \"train_cv_model_results.csv\"), index=False)\n",
    "        mlflow.log_artifact(os.path.join(output_dir, \"train_cv_model_results.csv\"), artifact_path=f\"outputs/train/metrics/{name_path}\")\n",
    "        df_model_results_best_model = df_model_results[df_model_results['model_rank']==1].copy()\n",
    "        best_auc_score_mean = df_model_results_best_model['auc_score_mean'].values[0]\n",
    "        mlflow.log_metric(f\"best_cv_train_{name_path}_auc_score_mean\", best_auc_score_mean)\n",
    "        best_auc_score_std = df_model_results_best_model['auc_score_std'].values[0]\n",
    "        mlflow.log_metric(f\"best_cv_train_{name_path}_auc_score_std\", best_auc_score_std)\n",
    "        best_auc_score_cv = df_model_results_best_model['auc_score_cv'].values[0]\n",
    "        mlflow.log_metric(f\"best_cv_train_{name_path}_auc_score_cv\", best_auc_score_cv)\n",
    "        df_model_results_best_model.to_csv(os.path.join(train_s3_path, \"outputs\", \"train\", \"metrics\", name_path, \"train_cv_model_results_best_model.csv\"), index=False)\n",
    "        df_model_results_best_model.to_csv(os.path.join(output_dir, \"train_cv_model_results_best_model.csv\"), index=False)\n",
    "        mlflow.log_artifact(os.path.join(output_dir, \"train_cv_model_results_best_model.csv\"), artifact_path=f\"outputs/train/metrics/{name_path}\")\n",
    "        df_feature_importance = pd.DataFrame({'variable': grid_search.feature_names_in_, 'importance': grid_search.best_estimator_.feature_importances_})\n",
    "        df_feature_importance.to_csv(os.path.join(train_s3_path, \"outputs\", \"train\", \"feature_importance\", name_path, \"feature_importance.csv\"), index=False)\n",
    "        df_feature_importance.to_csv(os.path.join(output_dir, \"feature_importance.csv\"), index=False)\n",
    "        mlflow.log_artifact(os.path.join(output_dir, \"feature_importance.csv\"), artifact_path=f\"outputs/train/feature_importance/{name_path}\")\n",
    "        with open(os.path.join(output_dir, \"grid_search_model.pickle\"), 'wb') as handle:\n",
    "            pickle.dump(grid_search, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        mlflow.log_artifact(os.path.join(output_dir, \"grid_search_model.pickle\"), artifact_path=f\"outputs/train/models/{name_path}\")\n",
    "        wr.s3.upload(local_file=os.path.join(output_dir, \"grid_search_model.pickle\"),path=os.path.join(train_s3_path, \"outputs\", \"train\", \"models\", name_path, \"grid_search_model.pickle\"))\n",
    "        \n",
    "    with mlflow.start_run(run_id=run_id):\n",
    "        with mlflow.start_run(run_name=\"ModelTraining\", nested=True) as training_run:\n",
    "            training_run_id = training_run.info.run_id\n",
    "            \n",
    "            model_parameters_grid_xgbost = {'max_depth': [3, 5],'eta': [0.05, 0.1],'gamma': [0, 1],'min_child_weight': [1, 5],'subsample': [0.8],'n_estimators': [50],'scale_pos_weight': [1, 5]}\n",
    "            xgb_model = XGBClassifier(objective='binary:logistic', eval_metric='auc',random_state=42)\n",
    "            mlflow.log_param(\"xgbost_param_grid\", str(model_parameters_grid_xgbost))\n",
    "            model_parameters_grid_random_forest = {'n_estimators': [100],'max_depth': [None, 6],'min_samples_leaf': [1, 10],'min_impurity_decrease': [0.0, 0.01]}\n",
    "            rf_model = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "            mlflow.log_param(\"random_forest_param_grid\", str(model_parameters_grid_random_forest))\n",
    "            clientes_train_path = download_artifacts(run_id=data_pull_id, artifact_path=\"data/out/data_train_prepared.csv\")\n",
    "            df_data_train = pd.read_csv(clientes_train_path)\n",
    "            path_train_clientes = os.path.join(train_s3_path, \"data\", \"out\", \"data_train_prepared.csv\")\n",
    "            mlflow.log_input(mlflow.data.from_pandas(df_data_train, path_train_clientes, targets='ATTRITION'), context=\"ModelTraining_data_train_cv\")\n",
    "            train_evaluate_models(df_data_train, model_parameters_grid_xgbost, xgb_model, 'xgbost')\n",
    "            train_evaluate_models(df_data_train, model_parameters_grid_random_forest, rf_model, 'random_forest')\n",
    "            \n",
    "    return training_run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b38b03-6072-4c4c-afbf-863c94a657cb",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2256fd18-23ac-4f13-8844-6b5eb653cfd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-29T15:24:13.651789Z",
     "iopub.status.busy": "2025-06-29T15:24:13.651035Z",
     "iopub.status.idle": "2025-06-29T15:24:13.669658Z",
     "shell.execute_reply": "2025-06-29T15:24:13.668962Z",
     "shell.execute_reply.started": "2025-06-29T15:24:13.651762Z"
    }
   },
   "outputs": [],
   "source": [
    "@step(\n",
    "    name=\"ModelEvaluation\",\n",
    "    instance_type=instance_type\n",
    ")\n",
    "def evaluate(experiment_name: str, run_id: str, data_pull_id: str, training_run_id: str) -> tuple[str, str, str]:\n",
    "    import subprocess\n",
    "    subprocess.run(['pip', 'install', 'awswrangler==3.12.0']) \n",
    "    import awswrangler as wr\n",
    "    import mlflow\n",
    "    from mlflow.artifacts import download_artifacts\n",
    "    import pandas as pd\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from xgboost import XGBClassifier\n",
    "    import os\n",
    "    import pickle\n",
    "    import tempfile\n",
    "    from sklearn.metrics import classification_report, roc_auc_score, RocCurveDisplay, log_loss, ConfusionMatrixDisplay, roc_curve\n",
    "    import matplotlib.pyplot as plt\n",
    "    import mlflow.sklearn\n",
    "    import mlflow.xgboost\n",
    "    from mlflow.models.signature import infer_signature\n",
    "    from mlflow.tracking import MlflowClient\n",
    "    output_dir = tempfile.mkdtemp()\n",
    "    mlflow.set_tracking_uri(tracking_server_arn)\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    train_s3_path = f\"s3://{default_path}\"\n",
    "    \n",
    "    # Funciones Best model\n",
    "\n",
    "    def get_features_name(name_path,training_run_id):\n",
    "        feature_path = download_artifacts(run_id=training_run_id, artifact_path=f'outputs/train/feature_importance/{name_path}/feature_importance.csv')\n",
    "        df_feature_importance = pd.read_csv(feature_path)    \n",
    "        return df_feature_importance['variable'].to_list()\n",
    "\n",
    "    def get_target_name():\n",
    "        y_col = \"ATTRITION\"\n",
    "        return y_col\n",
    "\n",
    "    def evaluate_best_model_in_dataset(df_data,name_path,best_model,training_run_id):\n",
    "        x_cols = get_features_name(name_path,training_run_id)\n",
    "        y_col = get_target_name()\n",
    "        y_proba = best_model.predict_proba(df_data[x_cols])[:,1]\n",
    "        \n",
    "        fpr, tpr, thresholds = roc_curve(df_data[y_col], y_proba)\n",
    "        j_scores = tpr - fpr\n",
    "        optimal_idx = j_scores.argmax()\n",
    "        best_threshold = thresholds[optimal_idx]\n",
    "        y_pred_label = (y_proba >= best_threshold).astype(int)\n",
    "\n",
    "        report = classification_report(df_data[y_col], y_pred_label, output_dict=True)\n",
    "        auc_metric = roc_auc_score(df_data[y_col],y_proba)\n",
    "        mlflow.log_metric(f\"{name_path}_roc_auc_test\", auc_metric)\n",
    "        mlflow.log_metric(f\"{name_path}_precision_test\", report[\"1\"][\"precision\"])\n",
    "        mlflow.log_metric(f\"{name_path}_recall_test\", report[\"1\"][\"recall\"])\n",
    "        mlflow.log_metric(f\"{name_path}_f1-score_test\", report[\"1\"][\"f1-score\"])\n",
    "        mlflow.log_metric(f\"{name_path}_log_loss_test\", log_loss(df_data[y_col], y_proba))\n",
    "\n",
    "        roc_display = RocCurveDisplay.from_predictions(df_data[y_col], y_proba)\n",
    "        plt.title(f\"Receiver Operating Characteristic (ROC) Curve for {name_path}\")\n",
    "        plt.savefig(os.path.join(output_dir,f'roc_curve_{name_path}_test.png'))\n",
    "        plt.close()\n",
    "        mlflow.log_artifact(os.path.join(output_dir, f\"roc_curve_{name_path}_test.png\"), artifact_path=f\"outputs/train/metrics/{name_path}\")\n",
    "        wr.s3.upload(local_file=os.path.join(output_dir, f\"roc_curve_{name_path}_test.png\"),path=os.path.join(train_s3_path, \"outputs\", \"train\", \"metrics\", name_path, f\"roc_curve_{name_path}_test.png\"))\n",
    "        \n",
    "        disp = ConfusionMatrixDisplay.from_predictions(df_data[y_col], y_pred_label)\n",
    "        plt.title(f\"Confusion Matrix for {name_path}\")\n",
    "        plt.savefig(os.path.join(output_dir,f'conf_matrix_{name_path}_test.png'))\n",
    "        plt.close()\n",
    "        mlflow.log_artifact(os.path.join(output_dir, f\"conf_matrix_{name_path}_test.png\"), artifact_path=f\"outputs/train/metrics/{name_path}\")\n",
    "        wr.s3.upload(local_file=os.path.join(output_dir, f\"conf_matrix_{name_path}_test.png\"),path=os.path.join(train_s3_path, \"outputs\", \"train\", \"metrics\", name_path, f\"conf_matrix_{name_path}_test.png\"))\n",
    "        \n",
    "        return report[\"1\"][\"recall\"]\n",
    "\n",
    "    def select_best_model(df_data_train, df_data_test,name_path,training_run_id):\n",
    "        path_grid = download_artifacts(run_id=training_run_id, artifact_path=f'outputs/train/models/{name_path}/grid_search_model.pickle')\n",
    "        with open(path_grid, 'rb') as handle:\n",
    "            grid_search = pickle.load(handle)\n",
    "\n",
    "        best_params = grid_search.best_params_\n",
    "        if name_path == \"xgbost\":\n",
    "            model = XGBClassifier(**best_params, objective='binary:logistic', eval_metric='auc', random_state=42)\n",
    "        elif name_path == \"random_forest\":\n",
    "            model = RandomForestClassifier(**best_params, random_state=42, class_weight='balanced')\n",
    "        \n",
    "        x_cols = get_features_name(name_path,training_run_id)\n",
    "        y_col = get_target_name()\n",
    "        model.fit(df_data_train[x_cols], df_data_train[y_col].values.ravel())\n",
    "        best_model = model\n",
    "        signature = infer_signature(df_data_train[x_cols], best_model.predict_proba(df_data_train[x_cols]))\n",
    "        input_example = df_data_train[x_cols].iloc[:5]\n",
    "        if name_path == \"xgbost\":\n",
    "            mlflow.xgboost.log_model(best_model, artifact_path=f\"{name_path}_model\", signature=signature,input_example=input_example)\n",
    "        elif name_path == \"random_forest\":\n",
    "            mlflow.sklearn.log_model(best_model, artifact_path=f\"{name_path}_model\", signature=signature,input_example=input_example)\n",
    "\n",
    "        with open(os.path.join(output_dir, \"best_model.pickle\"), 'wb') as handle:\n",
    "            pickle.dump(grid_search, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        mlflow.log_artifact(os.path.join(output_dir, \"best_model.pickle\"), artifact_path=f\"outputs/train/models/{name_path}\")\n",
    "        wr.s3.upload(local_file=os.path.join(output_dir, \"best_model.pickle\"),path=os.path.join(train_s3_path, \"outputs\", \"train\", \"models\", name_path, \"best_model.pickle\"))\n",
    "        \n",
    "        recall_metric_test = evaluate_best_model_in_dataset(df_data_test,name_path,best_model,training_run_id)\n",
    "        df_metrics = pd.DataFrame({'sample':['test'],'recall':[recall_metric_test]})\n",
    "\n",
    "        df_metrics.to_csv(os.path.join(train_s3_path, \"outputs\", \"train\", \"metrics\", name_path, \"test_metrics.csv\"), index=False)\n",
    "        df_metrics.to_csv(os.path.join(output_dir, \"test_metrics.csv\"), index=False)\n",
    "        mlflow.log_artifact(os.path.join(output_dir, \"test_metrics.csv\"), artifact_path=f\"outputs/train/metrics/{name_path}\")\n",
    "\n",
    "        return recall_metric_test\n",
    "\n",
    "    \n",
    "    with mlflow.start_run(run_id=run_id):\n",
    "        with mlflow.start_run(run_name=\"ModelEvaluation\", nested=True) as evaluation_run:\n",
    "            evaluation_run_id = evaluation_run.info.run_id\n",
    "            \n",
    "            clientes_train_path = download_artifacts(run_id=data_pull_id, artifact_path=\"data/out/data_train_prepared.csv\")\n",
    "            df_data_train = pd.read_csv(clientes_train_path)\n",
    "\n",
    "            path_train_clientes = os.path.join(train_s3_path, \"data\", \"out\", \"data_train_prepared.csv\")\n",
    "            mlflow.log_input(mlflow.data.from_pandas(df_data_train, path_train_clientes, targets='ATTRITION'), context=\"data_train\")\n",
    "            \n",
    "            clientes_test_path = download_artifacts(run_id=data_pull_id, artifact_path=\"data/out/data_test_prepared.csv\")\n",
    "            df_data_test = pd.read_csv(clientes_test_path)\n",
    "\n",
    "            path_test_clientes = os.path.join(train_s3_path, \"data\", \"out\", \"data_test_prepared.csv\")\n",
    "            mlflow.log_input(mlflow.data.from_pandas(df_data_test, path_test_clientes, targets='ATTRITION'), context=\"data_test\")\n",
    "            \n",
    "            metrics_rf = select_best_model(df_data_train, df_data_test, 'random_forest',training_run_id)\n",
    "            metrics_xg = select_best_model(df_data_train, df_data_test, 'xgbost',training_run_id)\n",
    "    return evaluation_run_id, metrics_rf, metrics_xg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba7b9e3-1df5-4ca0-b46a-c5525033ad72",
   "metadata": {},
   "source": [
    "## MODEL REGISTRATION RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1eff4b9c-489c-4706-86c7-343bf1a27517",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-29T15:24:14.908730Z",
     "iopub.status.busy": "2025-06-29T15:24:14.907979Z",
     "iopub.status.idle": "2025-06-29T15:24:14.915199Z",
     "shell.execute_reply": "2025-06-29T15:24:14.914529Z",
     "shell.execute_reply.started": "2025-06-29T15:24:14.908699Z"
    }
   },
   "outputs": [],
   "source": [
    "@step(\n",
    "    name=\"RegisterRandomForestModel\", \n",
    "    instance_type=instance_type \n",
    ")\n",
    "def register_random_forest_model(experiment_name: str, name_path: str, run_id: str, evaluation_run_id: str):\n",
    "\n",
    "    import mlflow\n",
    "    from mlflow.artifacts import download_artifacts\n",
    "    from mlflow.models.signature import infer_signature\n",
    "    from mlflow.tracking import MlflowClient\n",
    "    import os\n",
    "    import pickle\n",
    "    import pandas as pd\n",
    "    from xgboost import XGBClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    import tempfile\n",
    "    output_dir = tempfile.mkdtemp()\n",
    "    \n",
    "    mlflow.set_tracking_uri(tracking_server_arn)\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    \n",
    "    train_s3_path = f\"s3://{default_path}\"\n",
    "\n",
    "    with mlflow.start_run(run_id=run_id):\n",
    "        with mlflow.start_run(run_name=\"RegisterRandomForestModel\", nested=True):\n",
    "\n",
    "            model_uri = f\"runs:/{evaluation_run_id}/{name_path}_model\"\n",
    "            model_registry_name = \"attrition-detection-model\"\n",
    "            result = mlflow.register_model(model_uri=model_uri, name=model_registry_name)\n",
    "            client = MlflowClient()\n",
    "        \n",
    "            client.set_model_version_tag(\n",
    "            name=model_registry_name,\n",
    "            version=result.version,\n",
    "            key=\"estado\",\n",
    "            value=\"production\")\n",
    "\n",
    "            client.set_registered_model_alias(\n",
    "            name=model_registry_name,\n",
    "            alias=\"champion\",\n",
    "            version=result.version)\n",
    "\n",
    "            client.update_model_version(\n",
    "            name=model_registry_name,\n",
    "            version=result.version,\n",
    "            description=f\"{name_path} fue el modelo que obtuvo mejor recall usando los mejores hiperparametros por lo que ahora sera el modelo productivo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b4b080-38bd-4690-8301-6318904c344e",
   "metadata": {},
   "source": [
    "## MODEL REGISTRATION XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2091cba-ac46-444f-be39-27544f8964fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-29T15:24:16.109625Z",
     "iopub.status.busy": "2025-06-29T15:24:16.109149Z",
     "iopub.status.idle": "2025-06-29T15:24:16.115845Z",
     "shell.execute_reply": "2025-06-29T15:24:16.115043Z",
     "shell.execute_reply.started": "2025-06-29T15:24:16.109502Z"
    }
   },
   "outputs": [],
   "source": [
    "@step(\n",
    "    name=\"RegisterXGBoostModel\", \n",
    "    instance_type=instance_type\n",
    ")\n",
    "def register_xgboost_model(experiment_name: str, name_path: str, run_id: str, evaluation_run_id: str):\n",
    "    import mlflow\n",
    "    from mlflow.artifacts import download_artifacts\n",
    "    from mlflow.models.signature import infer_signature\n",
    "    from mlflow.tracking import MlflowClient\n",
    "    import os\n",
    "    import pickle\n",
    "    import pandas as pd\n",
    "    from xgboost import XGBClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    import tempfile\n",
    "    output_dir = tempfile.mkdtemp()\n",
    "    \n",
    "    mlflow.set_tracking_uri(tracking_server_arn)\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    \n",
    "    train_s3_path = f\"s3://{default_path}\"\n",
    "    with mlflow.start_run(run_id=run_id):\n",
    "        with mlflow.start_run(run_name=\"RegisterXGBoostModel\", nested=True):\n",
    "            \n",
    "            model_uri = f\"runs:/{evaluation_run_id}/{name_path}_model\"\n",
    "            model_registry_name = \"attrition-detection-model\"\n",
    "            result = mlflow.register_model(model_uri=model_uri, name=model_registry_name)\n",
    "            client = MlflowClient()\n",
    "        \n",
    "            client.set_model_version_tag(\n",
    "            name=model_registry_name,\n",
    "            version=result.version,\n",
    "            key=\"estado\",\n",
    "            value=\"production\")\n",
    "\n",
    "            client.set_registered_model_alias(\n",
    "            name=model_registry_name,\n",
    "            alias=\"champion\",\n",
    "            version=result.version)\n",
    "\n",
    "            client.update_model_version(\n",
    "            name=model_registry_name,\n",
    "            version=result.version,\n",
    "            description=f\"{name_path} fue el modelo que obtuvo mejor recall usando los mejores hiperparametros por lo que ahora sera el modelo productivo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32ecdb2-98db-40d7-bdde-4b16eac47dde",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3ee6f2f-1feb-4483-b9c9-49d7758d3257",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-29T15:24:17.397842Z",
     "iopub.status.busy": "2025-06-29T15:24:17.397311Z",
     "iopub.status.idle": "2025-06-29T15:24:17.428678Z",
     "shell.execute_reply": "2025-06-29T15:24:17.427749Z",
     "shell.execute_reply.started": "2025-06-29T15:24:17.397815Z"
    }
   },
   "outputs": [],
   "source": [
    "data_pull_step = data_pull(\n",
    "    experiment_name=experiment_name,\n",
    "    run_name=ExecutionVariables.PIPELINE_EXECUTION_ID,\n",
    "    cod_month=cod_month,\n",
    "    cod_month_start=cod_month_start,\n",
    "    cod_month_end=cod_month_end\n",
    ")\n",
    "\n",
    "model_training_step = model_training(\n",
    "    experiment_name=experiment_name,\n",
    "    run_id=data_pull_step[0],\n",
    "    data_pull_id=data_pull_step[1]\n",
    ")\n",
    "\n",
    "model_evaluation_step = evaluate(\n",
    "    experiment_name=experiment_name,\n",
    "    run_id=data_pull_step[0],\n",
    "    data_pull_id=data_pull_step[1],\n",
    "    training_run_id=model_training_step\n",
    ")\n",
    "\n",
    "conditional_register_step = ConditionStep(\n",
    "    name=\"ConditionalRegisterOverall\",\n",
    "    conditions=[\n",
    "        ConditionGreaterThanOrEqualTo(\n",
    "            left=model_evaluation_step[1], \n",
    "            right=model_evaluation_step[2], \n",
    "        )\n",
    "    ],\n",
    "    if_steps=[\n",
    "        ConditionStep(\n",
    "            name=\"ConditionalRegisterRandomForestBranch\",\n",
    "            conditions=[\n",
    "                ConditionGreaterThanOrEqualTo(\n",
    "                    left=model_evaluation_step[1], \n",
    "                    right=0.6,\n",
    "                )\n",
    "            ],\n",
    "            if_steps=[\n",
    "                register_random_forest_model( \n",
    "                    experiment_name=experiment_name,\n",
    "                    name_path='random_forest',\n",
    "                    run_id=data_pull_step[0],\n",
    "                    evaluation_run_id = model_evaluation_step[0]\n",
    "                )\n",
    "            ],\n",
    "            else_steps=[\n",
    "                FailStep(\n",
    "                    name=\"FailRandomForestPerformance\", \n",
    "                    error_message=\"Random Forest performance is not good enough\"\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    ],\n",
    "    else_steps=[\n",
    "        ConditionStep(\n",
    "            name=\"ConditionalRegisterXGBoostBranch\",\n",
    "            conditions=[\n",
    "                ConditionGreaterThanOrEqualTo(\n",
    "                    left=model_evaluation_step[2], \n",
    "                    right=0.6,\n",
    "                )\n",
    "            ],\n",
    "            if_steps=[\n",
    "                register_xgboost_model(\n",
    "                    experiment_name=experiment_name,\n",
    "                    name_path='xgbost', \n",
    "                    run_id=data_pull_step[0],\n",
    "                    evaluation_run_id = model_evaluation_step[0]\n",
    "                )\n",
    "            ],\n",
    "            else_steps=[\n",
    "                FailStep(\n",
    "                    name=\"FailXGBoostPerformance\", \n",
    "                    error_message=\"XGBoost performance is not good enough\"\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bff97edb-f6ea-4b01-8f34-e8bb1eea45bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-29T15:24:19.101346Z",
     "iopub.status.busy": "2025-06-29T15:24:19.101055Z",
     "iopub.status.idle": "2025-06-29T15:24:21.712272Z",
     "shell.execute_reply": "2025-06-29T15:24:21.711602Z",
     "shell.execute_reply.started": "2025-06-29T15:24:19.101325Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 15:24:19,467 sagemaker.remote_function INFO     Uploading serialized function code to s3://sagemaker-us-east-2-762233743642/pipeline-train/DataPull/2025-06-29-15-24-19-254/function\n",
      "2025-06-29 15:24:19,544 sagemaker.remote_function INFO     Uploading serialized function arguments to s3://sagemaker-us-east-2-762233743642/pipeline-train/DataPull/2025-06-29-15-24-19-254/arguments\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "2025-06-29 15:24:19,749 sagemaker.remote_function INFO     Uploading serialized function code to s3://sagemaker-us-east-2-762233743642/pipeline-train/ModelTraining/2025-06-29-15-24-19-254/function\n",
      "2025-06-29 15:24:19,802 sagemaker.remote_function INFO     Uploading serialized function arguments to s3://sagemaker-us-east-2-762233743642/pipeline-train/ModelTraining/2025-06-29-15-24-19-254/arguments\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "2025-06-29 15:24:19,876 sagemaker.remote_function INFO     Uploading serialized function code to s3://sagemaker-us-east-2-762233743642/pipeline-train/ModelEvaluation/2025-06-29-15-24-19-254/function\n",
      "2025-06-29 15:24:19,935 sagemaker.remote_function INFO     Uploading serialized function arguments to s3://sagemaker-us-east-2-762233743642/pipeline-train/ModelEvaluation/2025-06-29-15-24-19-254/arguments\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "2025-06-29 15:24:20,010 sagemaker.remote_function INFO     Uploading serialized function code to s3://sagemaker-us-east-2-762233743642/pipeline-train/RegisterRandomForestModel/2025-06-29-15-24-19-254/function\n",
      "2025-06-29 15:24:20,067 sagemaker.remote_function INFO     Uploading serialized function arguments to s3://sagemaker-us-east-2-762233743642/pipeline-train/RegisterRandomForestModel/2025-06-29-15-24-19-254/arguments\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "2025-06-29 15:24:20,130 sagemaker.remote_function INFO     Uploading serialized function code to s3://sagemaker-us-east-2-762233743642/pipeline-train/RegisterXGBoostModel/2025-06-29-15-24-19-254/function\n",
      "2025-06-29 15:24:20,185 sagemaker.remote_function INFO     Uploading serialized function arguments to s3://sagemaker-us-east-2-762233743642/pipeline-train/RegisterXGBoostModel/2025-06-29-15-24-19-254/arguments\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "2025-06-29 15:24:20,614 sagemaker.remote_function INFO     Uploading serialized function code to s3://sagemaker-us-east-2-762233743642/pipeline-train/DataPull/2025-06-29-15-24-20-613/function\n",
      "2025-06-29 15:24:20,703 sagemaker.remote_function INFO     Uploading serialized function arguments to s3://sagemaker-us-east-2-762233743642/pipeline-train/DataPull/2025-06-29-15-24-20-613/arguments\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "2025-06-29 15:24:20,882 sagemaker.remote_function INFO     Uploading serialized function code to s3://sagemaker-us-east-2-762233743642/pipeline-train/ModelTraining/2025-06-29-15-24-20-613/function\n",
      "2025-06-29 15:24:20,937 sagemaker.remote_function INFO     Uploading serialized function arguments to s3://sagemaker-us-east-2-762233743642/pipeline-train/ModelTraining/2025-06-29-15-24-20-613/arguments\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "2025-06-29 15:24:21,003 sagemaker.remote_function INFO     Uploading serialized function code to s3://sagemaker-us-east-2-762233743642/pipeline-train/ModelEvaluation/2025-06-29-15-24-20-613/function\n",
      "2025-06-29 15:24:21,068 sagemaker.remote_function INFO     Uploading serialized function arguments to s3://sagemaker-us-east-2-762233743642/pipeline-train/ModelEvaluation/2025-06-29-15-24-20-613/arguments\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "2025-06-29 15:24:21,130 sagemaker.remote_function INFO     Uploading serialized function code to s3://sagemaker-us-east-2-762233743642/pipeline-train/RegisterRandomForestModel/2025-06-29-15-24-20-613/function\n",
      "2025-06-29 15:24:21,191 sagemaker.remote_function INFO     Uploading serialized function arguments to s3://sagemaker-us-east-2-762233743642/pipeline-train/RegisterRandomForestModel/2025-06-29-15-24-20-613/arguments\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "2025-06-29 15:24:21,265 sagemaker.remote_function INFO     Uploading serialized function code to s3://sagemaker-us-east-2-762233743642/pipeline-train/RegisterXGBoostModel/2025-06-29-15-24-20-613/function\n",
      "2025-06-29 15:24:21,325 sagemaker.remote_function INFO     Uploading serialized function arguments to s3://sagemaker-us-east-2-762233743642/pipeline-train/RegisterXGBoostModel/2025-06-29-15-24-20-613/arguments\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-2:762233743642:pipeline/pipeline-train',\n",
       " 'ResponseMetadata': {'RequestId': '9c903aa1-5b8f-4aed-8b05-edf2f5722071',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '9c903aa1-5b8f-4aed-8b05-edf2f5722071',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '82',\n",
       "   'date': 'Sun, 29 Jun 2025 15:24:21 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    steps=[\n",
    "        data_pull_step,\n",
    "        model_training_step,\n",
    "        model_evaluation_step,\n",
    "        conditional_register_step\n",
    "    ],\n",
    "    parameters=[cod_month, cod_month_start, cod_month_end]\n",
    ")\n",
    "\n",
    "pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bca06abe-55ad-4117-a624-efe633ba9ded",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-29T15:24:23.492040Z",
     "iopub.status.busy": "2025-06-29T15:24:23.491695Z",
     "iopub.status.idle": "2025-06-29T15:24:23.725249Z",
     "shell.execute_reply": "2025-06-29T15:24:23.724545Z",
     "shell.execute_reply.started": "2025-06-29T15:24:23.492014Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_PipelineExecution(arn='arn:aws:sagemaker:us-east-2:762233743642:pipeline/pipeline-train/execution/7h0xjbe88zx5', sagemaker_session=<sagemaker.session.Session object at 0x7f0557bc42c0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.start(parameters={\"PeriodoCargaClientes\": '201208',\n",
    "                           \"PeriodoCargaRequerimientosInicio\": 201203,\n",
    "                          \"PeriodoCargaRequerimientosFin\": 201208},\n",
    "               execution_display_name=\"test-training-full-2\",\n",
    "               execution_description=\"Testando training full 2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
